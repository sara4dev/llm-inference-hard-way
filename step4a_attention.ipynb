{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4a: Attention Fundamentals â€” The Database Query Analogy ğŸ¯\n",
        "\n",
        "Every word in a sentence needs context to be understood. \"Bank\" means something different in \"river bank\" vs \"bank account.\" Attention is the mechanism that lets each token look at other tokens and gather the context it needs â€” think of it like each token running a database query against all other tokens to find relevant information.\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. **The Problem** â€” Why isolated tokens aren't enough\n",
        "2. **Q, K, V: The Database Query Analogy** â€” Query, Key, Value as a lookup system\n",
        "3. **Scaled Dot-Product Attention** â€” The core algorithm step by step\n",
        "4. **Softmax: Weighted Load Balancing** â€” Converting scores to percentages\n",
        "5. **Single-Head Attention** â€” Putting it all together\n",
        "\n",
        "In **Step 4b**, we'll extend this to multi-head attention and add causal masking.\n",
        "\n",
        "---\n",
        "\n",
        "## The Problem: Tokens in Isolation Know Nothing\n",
        "\n",
        "After embeddings (Step 3), each token has a 768-dimensional vector. But here's the problem:\n",
        "\n",
        "```\n",
        "\"The server crashed because it ran out of memory\"\n",
        "     â†“\n",
        "Each token has its own embedding, but:\n",
        "  - \"it\" doesn't know it refers to \"server\"\n",
        "  - \"crashed\" doesn't know what crashed\n",
        "  - Each embedding is computed in isolation!\n",
        "```\n",
        "\n",
        "**Think of it like this:** Imagine you're a customer support agent handling a ticket. You can see ONE word of the ticket at a time. How would you understand \"Please restart it\"? You need context!\n",
        "\n",
        "**The solution: Self-Attention**\n",
        "\n",
        "Each token gets to \"query\" all other tokens and gather relevant context:\n",
        "\n",
        "```\n",
        "Token \"it\" asks: \"Who in this sentence am I referring to?\"\n",
        "                           â†“\n",
        "   Attention scores: [server: 0.85, crashed: 0.05, memory: 0.03, ...]\n",
        "                           â†“\n",
        "   \"it\" now carries information about \"server\" in its representation\n",
        "```\n",
        "\n",
        "This is exactly like a **database query** â€” you have a query, you search against keys, and you retrieve values. Let's see how.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Load Everything from Previous Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import regex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SafeTensors (from Step 1)\n",
        "def load_safetensors(path: str) -> dict:\n",
        "    \"\"\"Load a SafeTensors file without external library.\"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        header_size = int.from_bytes(f.read(8), 'little')\n",
        "        header_json = f.read(header_size).decode('utf-8')\n",
        "        header = json.loads(header_json)\n",
        "        header.pop(\"__metadata__\", {})\n",
        "        \n",
        "        tensors = {}\n",
        "        data_start = 8 + header_size\n",
        "        \n",
        "        for name, info in header.items():\n",
        "            dtype_map = {\"F32\": np.float32, \"F16\": np.float16, \"I64\": np.int64}\n",
        "            dtype = dtype_map.get(info[\"dtype\"], np.float32)\n",
        "            offset_start, offset_end = info[\"data_offsets\"]\n",
        "            \n",
        "            f.seek(data_start + offset_start)\n",
        "            raw_data = f.read(offset_end - offset_start)\n",
        "            tensors[name] = np.frombuffer(raw_data, dtype=dtype).reshape(info[\"shape\"])\n",
        "        \n",
        "        return tensors\n",
        "\n",
        "# Load model weights\n",
        "tensors = load_safetensors(\"models/gpt2/model.safetensors\")\n",
        "\n",
        "# Load config\n",
        "with open(\"models/gpt2/config.json\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(tensors)} weight tensors\")\n",
        "print(f\"\\nKey hyperparameters:\")\n",
        "print(f\"  n_vocab: {config['vocab_size']:,}\")\n",
        "print(f\"  n_embd:  {config['n_embd']}\")\n",
        "print(f\"  n_head:  {config['n_head']}\")\n",
        "print(f\"  n_layer: {config['n_layer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT-2 Tokenizer (from Step 2)\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"Create byte-to-unicode mapping for GPT-2.\"\"\"\n",
        "    printable = list(range(ord(\"!\"), ord(\"~\") + 1))\n",
        "    printable += list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1))\n",
        "    printable += list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n",
        "    \n",
        "    byte_values = printable[:]\n",
        "    unicode_chars = printable[:]\n",
        "    \n",
        "    n = 0\n",
        "    for b in range(256):\n",
        "        if b not in printable:\n",
        "            byte_values.append(b)\n",
        "            unicode_chars.append(256 + n)\n",
        "            n += 1\n",
        "    \n",
        "    return {b: chr(c) for b, c in zip(byte_values, unicode_chars)}\n",
        "\n",
        "\n",
        "class GPT2Tokenizer:\n",
        "    \"\"\"BPE tokenizer for GPT-2.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_path=\"models/gpt2/vocab.json\", merges_path=\"models/gpt2/merges.txt\"):\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.vocab = json.load(f)\n",
        "        self.vocab_inverse = {v: k for k, v in self.vocab.items()}\n",
        "        \n",
        "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            merges_text = f.read()\n",
        "        merge_lines = [line for line in merges_text.split(\"\\n\")[1:] if line]\n",
        "        self.bpe_merges = {}\n",
        "        for i, line in enumerate(merge_lines):\n",
        "            parts = line.split(\" \")\n",
        "            if len(parts) == 2:\n",
        "                self.bpe_merges[tuple(parts)] = i\n",
        "        \n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.pattern = regex.compile(\n",
        "            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        )\n",
        "        self.cache = {}\n",
        "    \n",
        "    def _get_pairs(self, word):\n",
        "        return set((word[i], word[i + 1]) for i in range(len(word) - 1))\n",
        "    \n",
        "    def _bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        \n",
        "        word = list(token)\n",
        "        while len(word) > 1:\n",
        "            pairs = self._get_pairs(word)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = min(pairs, key=lambda p: self.bpe_merges.get(p, float('inf')))\n",
        "            if best_pair not in self.bpe_merges:\n",
        "                break\n",
        "            first, second = best_pair\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "        \n",
        "        result = tuple(word)\n",
        "        self.cache[token] = result\n",
        "        return result\n",
        "    \n",
        "    def encode(self, text):\n",
        "        token_ids = []\n",
        "        for chunk in self.pattern.findall(text):\n",
        "            byte_encoded = ''.join(self.byte_encoder[b] for b in chunk.encode('utf-8'))\n",
        "            bpe_tokens = self._bpe(byte_encoded)\n",
        "            for token in bpe_tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, token_ids):\n",
        "        text = ''.join(self.vocab_inverse[tid] for tid in token_ids)\n",
        "        byte_values = [self.byte_decoder[c] for c in text]\n",
        "        return bytes(byte_values).decode('utf-8', errors='replace')\n",
        "    \n",
        "    def get_token_string(self, token_id):\n",
        "        return self.vocab_inverse.get(token_id, \"<UNK>\")\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer()\n",
        "print(f\"âœ… Tokenizer loaded with {len(tokenizer.vocab):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get embeddings (from Step 3)\n",
        "wte = tensors[\"wte.weight\"]  # Token embeddings: [50257, 768]\n",
        "wpe = tensors[\"wpe.weight\"]  # Position embeddings: [1024, 768]\n",
        "\n",
        "def get_input_embeddings(text):\n",
        "    \"\"\"Get the input embeddings for text (token + position).\"\"\"\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    seq_len = len(token_ids)\n",
        "    token_emb = wte[token_ids]\n",
        "    position_emb = wpe[:seq_len]\n",
        "    return token_emb + position_emb, token_ids\n",
        "\n",
        "# Test\n",
        "test_emb, test_ids = get_input_embeddings(\"Hello world\")\n",
        "print(f\"âœ… Embedding function ready\")\n",
        "print(f\"   'Hello world' -> shape {test_emb.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Q, K, V: Attention Works Like a Database Query\n",
        "\n",
        "If you've ever used a key-value store like Redis or queried a database, you already understand the core concept of attention!\n",
        "\n",
        "### The Database Analogy\n",
        "\n",
        "Think of attention as each token running a query against a database:\n",
        "\n",
        "| Attention Term | Database Analogy | What It Does |\n",
        "|----------------|------------------|--------------|\n",
        "| **Query (Q)** | Your SQL SELECT query | \"What am I looking for?\" |\n",
        "| **Key (K)** | Indexed columns to search | \"Here's what each row contains\" |\n",
        "| **Value (V)** | The actual data in rows | \"Here's the information to retrieve\" |\n",
        "\n",
        "**Concrete Example:**\n",
        "\n",
        "Imagine a Redis-like key-value store where each token is a row:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Token Database                                                 â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Token   â”‚  Key (searchable)   â”‚  Value (retrievable data)     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  \"The\"   â”‚  [0.2, -0.1, ...]   â”‚  [0.5, 0.3, ...]              â”‚\n",
        "â”‚  \"server\"â”‚  [0.8, 0.3, ...]    â”‚  [0.7, 0.1, ...]  â† relevant! â”‚\n",
        "â”‚  \"crashed\"â”‚ [0.4, 0.1, ...]    â”‚  [0.2, 0.8, ...]              â”‚\n",
        "â”‚  \"it\"    â”‚  [0.7, 0.2, ...]    â”‚  [0.1, 0.4, ...]              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "When token \"it\" wants to understand its context, it:\n",
        "\n",
        "1. **Creates a Query**: \"it\" generates a query vector: `Q_it = [0.75, 0.25, ...]`\n",
        "2. **Searches all Keys**: Compare Q_it with each token's Key using dot product\n",
        "3. **Gets relevance scores**: `{The: 0.1, server: 0.85, crashed: 0.03, it: 0.02}`\n",
        "4. **Retrieves weighted Values**: Combine Values weighted by those scores\n",
        "\n",
        "The result? Token \"it\" now has information about \"server\" mixed into its representation!\n",
        "\n",
        "```\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "     \"it\" Query â”€â”€â”€â–ºâ”‚  Search Keys     â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                             â”‚ Match scores\n",
        "                             â–¼\n",
        "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "              â”‚  server: 0.85  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚\n",
        "              â”‚  The:    0.10  â–ˆ             â”‚\n",
        "              â”‚  crashed: 0.03               â”‚\n",
        "              â”‚  it:     0.02                â”‚\n",
        "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                             â”‚\n",
        "                             â–¼ Weighted retrieval\n",
        "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "              â”‚  Output = 0.85 Ã— V_server    â”‚\n",
        "              â”‚         + 0.10 Ã— V_The       â”‚\n",
        "              â”‚         + 0.03 Ã— V_crashed   â”‚\n",
        "              â”‚         + 0.02 Ã— V_it        â”‚\n",
        "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Why Three Separate Projections?\n",
        "\n",
        "You might ask: \"Why not use the same vector for Query, Key, and Value?\"\n",
        "\n",
        "Using different projections lets the model learn:\n",
        "- **What to search for** (Query) â€” might emphasize semantic meaning\n",
        "- **What to advertise** (Key) â€” might emphasize grammatical role  \n",
        "- **What to provide** (Value) â€” might emphasize the actual information content\n",
        "\n",
        "It's like having separate indexes optimized for searching vs. the actual data you retrieve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GPT-2's Attention Weights: Under the Hood\n",
        "\n",
        "Now let's look at how GPT-2 actually stores its attention weights. Here's a clever optimization:\n",
        "\n",
        "**Instead of 3 separate weight matrices (W_q, W_k, W_v), GPT-2 combines them into ONE matrix!**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  c_attn.weight: [768, 2304]                                    â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
        "â”‚  â”‚   W_query    â”‚    W_key     â”‚   W_value    â”‚                â”‚\n",
        "â”‚  â”‚  [768, 768]  â”‚  [768, 768]  â”‚  [768, 768]  â”‚                â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
        "â”‚         â†‘              â†‘              â†‘                        â”‚\n",
        "â”‚    cols 0-767     cols 768-1535  cols 1536-2303                â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "Why combine them? **Efficiency.** One matrix multiplication is faster than three separate ones. The GPU can process it in a single operation, then we split the result.\n",
        "\n",
        "This is a common pattern in production systems: batch multiple operations together for better throughput.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at attention weights from the first layer\n",
        "c_attn_weight = tensors[\"h.0.attn.c_attn.weight\"]\n",
        "c_attn_bias = tensors[\"h.0.attn.c_attn.bias\"]\n",
        "c_proj_weight = tensors[\"h.0.attn.c_proj.weight\"]\n",
        "c_proj_bias = tensors[\"h.0.attn.c_proj.bias\"]\n",
        "\n",
        "print(\"Attention weights (Layer 0):\")\n",
        "print(f\"  c_attn.weight: {c_attn_weight.shape}  <- Creates Q, K, V\")\n",
        "print(f\"  c_attn.bias:   {c_attn_bias.shape}\")\n",
        "print(f\"  c_proj.weight: {c_proj_weight.shape}  <- Output projection\")\n",
        "print(f\"  c_proj.bias:   {c_proj_bias.shape}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ c_attn.weight has 2304 = 768 Ã— 3 columns\")\n",
        "print(f\"   First 768 cols â†’ Q weights\")\n",
        "print(f\"   Next 768 cols â†’ K weights\")\n",
        "print(f\"   Last 768 cols â†’ V weights\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Q, K, V weight matrices\n",
        "n_embd = config[\"n_embd\"]  # 768\n",
        "\n",
        "# Split the combined weight matrix\n",
        "W_q = c_attn_weight[:, :n_embd]        # [768, 768]\n",
        "W_k = c_attn_weight[:, n_embd:2*n_embd]  # [768, 768]\n",
        "W_v = c_attn_weight[:, 2*n_embd:]       # [768, 768]\n",
        "\n",
        "# Split the combined bias\n",
        "b_q = c_attn_bias[:n_embd]\n",
        "b_k = c_attn_bias[n_embd:2*n_embd]\n",
        "b_v = c_attn_bias[2*n_embd:]\n",
        "\n",
        "print(\"Separated Q, K, V weight matrices:\")\n",
        "print(f\"  W_q: {W_q.shape}\")\n",
        "print(f\"  W_k: {W_k.shape}\")\n",
        "print(f\"  W_v: {W_v.shape}\")\n",
        "\n",
        "print(\"Separated Q, K, V bias vectors:\")\n",
        "print(f\"  b_q: {b_q.shape}\")\n",
        "print(f\"  b_k: {b_k.shape}\")\n",
        "print(f\"  b_v: {b_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Computing Q, K, V: Matrix Multiplication in Action\n",
        "\n",
        "Now we compute the Query, Key, and Value vectors for each token. Each token's embedding goes through a **linear transformation** (matrix multiplication + bias):\n",
        "\n",
        "```\n",
        "Input X: (seq_len, 768) â€” our token embeddings from Step 3\n",
        "         â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Q = X @ W_q + b_q    â†’  (seq_len, 768)  \"What to search\"   â”‚\n",
        "â”‚  K = X @ W_k + b_k    â†’  (seq_len, 768)  \"What to advertise\"â”‚\n",
        "â”‚  V = X @ W_v + b_v    â†’  (seq_len, 768)  \"What to provide\"  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**What's happening:** Each 768-dimensional embedding gets transformed into three different 768-dimensional vectors. The weight matrices are learned during training to create useful Q, K, V representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_qkv(\n",
        "    x: np.ndarray,       # (seq_len, n_embd) - input embeddings\n",
        "    W_q: np.ndarray,     # (n_embd, n_embd) - query weights\n",
        "    W_k: np.ndarray,     # (n_embd, n_embd) - key weights\n",
        "    W_v: np.ndarray,     # (n_embd, n_embd) - value weights\n",
        "    b_q: np.ndarray,     # (n_embd,) - query bias\n",
        "    b_k: np.ndarray,     # (n_embd,) - key bias\n",
        "    b_v: np.ndarray      # (n_embd,) - value bias\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute Query, Key, Value projections.\n",
        "    \n",
        "    This is like preparing database indexes:\n",
        "    - Q: \"What am I searching for?\"\n",
        "    - K: \"What can I be found by?\"\n",
        "    - V: \"What information do I contain?\"\n",
        "    \n",
        "    Returns:\n",
        "        Q, K, V each of shape (seq_len, n_embd)\n",
        "    \"\"\"\n",
        "    # Each is a simple linear transformation: matrix multiply + bias\n",
        "    Q = x @ W_q + b_q  # (seq_len, 768) @ (768, 768) + (768,) â†’ (seq_len, 768)\n",
        "    K = x @ W_k + b_k\n",
        "    V = x @ W_v + b_v\n",
        "    return Q, K, V\n",
        "\n",
        "\n",
        "# Test with \"Hello world\"\n",
        "text = \"Hello world\"\n",
        "x, token_ids = get_input_embeddings(text)\n",
        "\n",
        "Q, K, V = compute_qkv(x, W_q, W_k, W_v, b_q, b_k, b_v)\n",
        "\n",
        "print(f\"Input: '{text}'\")\n",
        "print(f\"  x shape: {x.shape} (input embeddings)\")\n",
        "print(f\"  Q shape: {Q.shape} (queries - 'what am I looking for?')\")\n",
        "print(f\"  K shape: {K.shape} (keys - 'what do I contain?')\")\n",
        "print(f\"  V shape: {V.shape} (values - 'what info do I provide?')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Peek at the Q, K, V values\n",
        "tokens = [tokenizer.get_token_string(tid).replace(\"Ä \", \"âµ\") for tid in token_ids]\n",
        "\n",
        "print(\"Q, K, V values (first 5 dimensions):\")\n",
        "print(\"=\" * 70)\n",
        "for i, tok in enumerate(tokens):\n",
        "    print(f\"\\nToken '{tok}' (position {i}):\")\n",
        "    print(f\"  Q: [{', '.join(f'{v:.3f}' for v in Q[i, :5])}...]\")\n",
        "    print(f\"  K: [{', '.join(f'{v:.3f}' for v in K[i, :5])}...]\")\n",
        "    print(f\"  V: [{', '.join(f'{v:.3f}' for v in V[i, :5])}...]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Scaled Dot-Product Attention: The Core Algorithm\n",
        "\n",
        "Now let's implement the attention mechanism step by step. Here's the complete pipeline:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  ATTENTION PIPELINE                                              â”‚\n",
        "â”‚                                                                  â”‚\n",
        "â”‚  Step 1: Q Ã— K^T  â†’  Raw match scores (how similar are they?)   â”‚\n",
        "â”‚            â†“                                                     â”‚\n",
        "â”‚  Step 2: Ã· âˆšd_k   â†’  Scale scores (prevent explosion)           â”‚\n",
        "â”‚            â†“                                                     â”‚\n",
        "â”‚  Step 3: Softmax  â†’  Convert to percentages (load balancing)    â”‚\n",
        "â”‚            â†“                                                     â”‚\n",
        "â”‚  Step 4: Ã— V      â†’  Weighted retrieval (get relevant info)     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "Let's implement each step.\n",
        "\n",
        "### Step 1: Q Ã— K^T â€” Computing Match Scores\n",
        "\n",
        "The dot product measures similarity between vectors. When we multiply Q with K transposed:\n",
        "\n",
        "```\n",
        "Q:   (seq_len, d_k)    â€” each row is a token's query\n",
        "K^T: (d_k, seq_len)    â€” each column is a token's key\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Q Ã— K^T: (seq_len, seq_len)  â€” scores[i,j] = how much token i matches token j\n",
        "```\n",
        "\n",
        "Think of it like computing a similarity matrix â€” each entry tells us how \"compatible\" one token's query is with another token's key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Compute QK^T\n",
        "attention_scores = Q @ K.T  # (seq_len, seq_len)\n",
        "\n",
        "print(\"Step 1: QK^T (raw attention scores)\")\n",
        "print(f\"  Shape: {attention_scores.shape}\")\n",
        "print(f\"\\n  Score matrix (token i attending to token j):\")\n",
        "print(f\"  Rows = query tokens, Cols = key tokens\")\n",
        "print()\n",
        "\n",
        "# Pretty print the matrix\n",
        "print(\"       \", end=\"\")\n",
        "for tok in tokens:\n",
        "    print(f\"{tok:>10}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, tok in enumerate(tokens):\n",
        "    print(f\"{tok:>6} \", end=\"\")\n",
        "    for j in range(len(tokens)):\n",
        "        print(f\"{attention_scores[i, j]:>10.2f}\", end=\"\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Scaling â€” Why Divide by âˆšd?\n",
        "\n",
        "**The Problem:** Look at those raw scores above â€” they're huge! When we pass large numbers to softmax, it \"saturates\" â€” almost all attention goes to one token (99.99%), others get nothing (0.01%).\n",
        "\n",
        "**Why do dot products get large?**\n",
        "\n",
        "The dot product sums up `d` component-wise multiplications. More components = larger sum (in magnitude).\n",
        "\n",
        "For vectors with components having variance ~1:\n",
        "- The **variance** of the dot product equals `d` (the dimension)\n",
        "- The **standard deviation** = âˆšd\n",
        "\n",
        "```\n",
        "Example with normalized random vectors:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        " Dimension   â”‚  Std Dev of Dot Product\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "     10      â”‚     âˆš10  â‰ˆ  3\n",
        "     64      â”‚     âˆš64  =  8    â† GPT-2 uses 64 dims per head\n",
        "    768      â”‚    âˆš768  â‰ˆ 28\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "```\n",
        "\n",
        "In practice, actual embeddings aren't normalized, so scores can be even larger (as we saw above: thousands!).\n",
        "\n",
        "**The Solution:** Divide by âˆšd_k to normalize the variance back to ~1:\n",
        "\n",
        "```\n",
        "scores = (Q @ K.T) / âˆšd_k\n",
        "\n",
        "Before: variance â‰ˆ d_k  â†’  After: variance â‰ˆ 1\n",
        "```\n",
        "\n",
        "This is similar to normalizing metrics before comparing them â€” you wouldn't compare \"requests per second\" directly with \"latency in milliseconds\" without scaling first!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For now, let's use full dimension (we'll do multi-head later)\n",
        "d_k = n_embd  # 768\n",
        "scale = np.sqrt(d_k)\n",
        "\n",
        "scaled_scores = attention_scores / scale\n",
        "\n",
        "print(f\"Step 2: Scale by âˆšd_k = âˆš{d_k} = {scale:.2f}\")\n",
        "print(f\"\\n  Before scaling - max: {attention_scores.max():.2f}, min: {attention_scores.min():.2f}\")\n",
        "print(f\"  After scaling  - max: {scaled_scores.max():.2f}, min: {scaled_scores.min():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Softmax â€” Weighted Load Balancing\n",
        "\n",
        "**Softmax is like a load balancer that distributes traffic based on server scores.**\n",
        "\n",
        "Imagine you're building a load balancer and have these server health scores:\n",
        "\n",
        "```\n",
        "Server A: 3    (fast, healthy)\n",
        "Server B: 2    (medium)\n",
        "Server C: 1    (slow)\n",
        "Server D: 0    (slowest)\n",
        "```\n",
        "\n",
        "You want to distribute traffic so that:\n",
        "- Higher scores â†’ more traffic\n",
        "- All percentages sum to 100%\n",
        "\n",
        "Softmax does exactly this:\n",
        "\n",
        "```\n",
        "Input scores:  [3, 2, 1, 0]   â† Server A, B, C, D\n",
        "               â†“ softmax\n",
        "Output %:      [64%, 24%, 9%, 3%]  â† sums to 100%!\n",
        "```\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. `exp(score)` â€” converts each score to a positive number (amplifies differences)\n",
        "2. Divide each by the sum â€” normalizes to percentages\n",
        "\n",
        "```python\n",
        "def softmax(scores):\n",
        "    exp_scores = np.exp(scores - scores.max())  # subtract max for stability\n",
        "    return exp_scores / exp_scores.sum()        # divide by sum to get percentages\n",
        "```\n",
        "\n",
        "**In attention:** Softmax answers \"What percentage of attention should token i pay to each other token?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute softmax â€” converts scores to percentages that sum to 100%.\n",
        "    \n",
        "    Think of it like a load balancer:\n",
        "    - Input: server health scores [3, 2, 1, 0]\n",
        "    - Output: traffic percentages [64%, 24%, 9%, 3%]\n",
        "    \n",
        "    The trick: subtract max before exp() to prevent numerical overflow.\n",
        "    This doesn't change the result because softmax(x) = softmax(x - c).\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability (prevents exp() from exploding)\n",
        "    x_shifted = x - x.max(axis=axis, keepdims=True)\n",
        "    \n",
        "    # exp() makes all values positive and amplifies differences\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    \n",
        "    # Divide by sum to get percentages\n",
        "    return exp_x / exp_x.sum(axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "attention_weights = softmax(scaled_scores, axis=-1)\n",
        "\n",
        "print(\"Step 3: Softmax (converts to attention percentages)\")\n",
        "print(f\"  Shape: {attention_weights.shape}\")\n",
        "print(f\"  Each row sums to: {attention_weights.sum(axis=-1)} (100% total attention)\")\n",
        "print()\n",
        "\n",
        "# Pretty print - like a load balancer traffic distribution\n",
        "print(\"Attention weights (each row = traffic distribution for that token):\")\n",
        "print()\n",
        "print(\"       \", end=\"\")\n",
        "for tok in tokens:\n",
        "    print(f\"{tok:>10}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, tok in enumerate(tokens):\n",
        "    print(f\"{tok:>6} \", end=\"\")\n",
        "    for j in range(len(tokens)):\n",
        "        weight = attention_weights[i, j]\n",
        "        # Visual bar shows relative weight\n",
        "        bar = \"â–ˆ\" * int(weight * 10)\n",
        "        print(f\"{weight:>6.2%} {bar:<4}\", end=\"\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Weighted Retrieval â€” Gathering Information\n",
        "\n",
        "Now we use those attention percentages to retrieve a weighted combination of Values.\n",
        "\n",
        "**Think of it like aggregating responses from multiple microservices:**\n",
        "\n",
        "```\n",
        "Service A returns: {\"latency\": 10, \"status\": \"ok\"}     weight: 0.85\n",
        "Service B returns: {\"latency\": 50, \"status\": \"slow\"}   weight: 0.10\n",
        "Service C returns: {\"latency\": 5, \"status\": \"fast\"}    weight: 0.05\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Weighted result:   0.85 Ã— A + 0.10 Ã— B + 0.05 Ã— C\n",
        "```\n",
        "\n",
        "In attention:\n",
        "```\n",
        "output = attention_weights @ V    # (seq_len, n_embd)\n",
        "\n",
        "For each token i:\n",
        "  output[i] = sum( attention_weight[i,j] Ã— V[j] for all tokens j )\n",
        "```\n",
        "\n",
        "Each output token is now a **context-aware blend** of all token Values, weighted by how relevant each one was!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Weighted sum of values\n",
        "attention_output = attention_weights @ V\n",
        "\n",
        "print(\"Step 4: Weighted sum of Values\")\n",
        "print(f\"  attention_weights: {attention_weights.shape}\")\n",
        "print(f\"  V:                 {V.shape}\")\n",
        "print(f\"  output:            {attention_output.shape}\")\n",
        "print()\n",
        "print(\"Each output token is now a context-aware combination of all tokens!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Putting It Together: Single-Head Attention\n",
        "\n",
        "Let's combine all four steps into a reusable function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attention(\n",
        "    Q: np.ndarray,                    # (seq_len, d_k) - queries\n",
        "    K: np.ndarray,                    # (seq_len, d_k) - keys  \n",
        "    V: np.ndarray,                    # (seq_len, d_v) - values\n",
        "    mask: np.ndarray = None           # (seq_len, seq_len) - optional causal mask\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Scaled dot-product attention â€” the core of transformers!\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Query-Key matching:  scores = Q Ã— K^T\n",
        "    2. Scale:              scores = scores / âˆšd_k\n",
        "    3. Mask (optional):    scores = scores + mask\n",
        "    4. Load balance:       weights = softmax(scores)\n",
        "    5. Weighted retrieval: output = weights Ã— V\n",
        "    \n",
        "    Returns:\n",
        "        output: Context-enriched representations (seq_len, d_v)\n",
        "        weights: Attention distribution (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]  # Dimension of keys/queries\n",
        "    \n",
        "    # Step 1 & 2: Compute similarity scores and scale\n",
        "    scores = (Q @ K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n",
        "    \n",
        "    # Step 3: Apply mask (for causal attention)\n",
        "    if mask is not None:\n",
        "        scores = scores + mask  # -inf positions become 0 after softmax\n",
        "    \n",
        "    # Step 4: Convert to attention percentages\n",
        "    weights = softmax(scores, axis=-1)  # Each row sums to 1\n",
        "    \n",
        "    # Step 5: Weighted retrieval of values\n",
        "    output = weights @ V  # (seq_len, d_v)\n",
        "    \n",
        "    return output, weights\n",
        "\n",
        "\n",
        "# Test it\n",
        "output, weights = attention(Q, K, V)\n",
        "print(f\"Attention output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"\\nEach token now carries context from other tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Test: Verify Our Implementation\n",
        "\n",
        "Let's verify our attention implementation has the expected properties:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quiz: Verify key properties\n",
        "print(\"ğŸ§ª Attention Concept Verification\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Attention weights sum to 1\n",
        "row_sums = weights.sum(axis=-1)  # Should all be 1.0\n",
        "print(f\"1. Attention weights sum to 1 per row: {np.allclose(row_sums, 1.0)} âœ“\")\n",
        "\n",
        "# 2. Output dimension matches input\n",
        "print(f\"2. Output shape equals input shape: {x.shape == output.shape} âœ“\")\n",
        "\n",
        "# 3. Q, K, V split correctly\n",
        "print(f\"3. QKV weight is 3x embedding dim: {c_attn_weight.shape[1] == 3 * n_embd} âœ“\")\n",
        "\n",
        "print()\n",
        "print(\"All attention properties verified! ğŸ‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: What We Learned\n",
        "\n",
        "### Attention = Database Query + Load Balancing\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  THE ATTENTION PIPELINE (Single-Head)                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                  â”‚\n",
        "â”‚  Input: Token embeddings (seq_len, 768)                          â”‚\n",
        "â”‚         â†“                                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚  1. QKV Projection (database query setup)                  â”‚  â”‚\n",
        "â”‚  â”‚     Q, K, V = split(x @ W_qkv + b_qkv)                     â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚         â†“                                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚  2. Attention:                                             â”‚  â”‚\n",
        "â”‚  â”‚     a. Q Ã— K^T          (query-key matching)               â”‚  â”‚\n",
        "â”‚  â”‚     b. Ã· âˆš768           (normalize scores)                 â”‚  â”‚\n",
        "â”‚  â”‚     c. softmax          (load balance weights)             â”‚  â”‚\n",
        "â”‚  â”‚     d. Ã— V              (weighted retrieval)               â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚         â†“                                                        â”‚\n",
        "â”‚  Output: Context-aware embeddings (seq_len, 768)                 â”‚\n",
        "â”‚                                                                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Key Concepts (with Infrastructure Analogies)\n",
        "\n",
        "| Concept | What It Does | Analogy |\n",
        "|---------|--------------|---------|\n",
        "| **Q, K, V** | Query-Key-Value projections | Database: Query searches Keys, retrieves Values |\n",
        "| **Dot Product** | Measures query-key similarity | Search relevance scoring |\n",
        "| **Scaling (Ã·âˆšd)** | Prevents score explosion | Normalizing metrics before comparison |\n",
        "| **Softmax** | Converts to percentages | Load balancer distributing traffic |\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Step 4b â€” Multi-Head Attention & Causal Masking\n",
        "\n",
        "We've built single-head attention, but GPT-2 uses **12 heads** running in parallel! In **Step 4b**, we'll learn:\n",
        "\n",
        "1. **Causal Masking** â€” How to prevent tokens from \"seeing the future\"\n",
        "2. **Multi-Head Attention** â€” Running 12 specialized attention operations in parallel\n",
        "3. **Visualization** â€” Seeing what different heads focus on\n",
        "4. **Complete Implementation** â€” The full `CausalSelfAttention` class\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  MULTI-HEAD ATTENTION (Preview of Step 4b)                      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Worker 1 (Head 0):  \"I focus on the previous word\"            â”‚\n",
        "â”‚  Worker 2 (Head 1):  \"I track grammatical subjects\"            â”‚\n",
        "â”‚  Worker 3 (Head 2):  \"I find pronoun references\"               â”‚\n",
        "â”‚  ...                                                            â”‚\n",
        "â”‚  Worker 12 (Head 11): \"I look for semantic relationships\"      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Final Output: Combine all worker results                       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
