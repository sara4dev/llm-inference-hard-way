{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Embeddings & Positional Encoding ðŸ“\n",
        "\n",
        "In this notebook, we'll understand how **token embeddings** and **positional embeddings** work together to give the transformer a rich representation of input text.\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. **Why Embeddings?** - Why we can't just use token IDs directly\n",
        "2. **Token Embeddings** - Convert token IDs â†’ dense vectors\n",
        "3. **Position Embeddings** - Encode where each token appears\n",
        "4. **Combining Embeddings** - The input to the transformer\n",
        "5. **Visualizing Embeddings** - See semantic relationships\n",
        "6. **Similarity Search** - Find related tokens using embeddings\n",
        "\n",
        "---\n",
        "\n",
        "## The Big Picture: From Text to Vectors\n",
        "\n",
        "We've already seen how tokenization converts text to token IDs:\n",
        "\n",
        "```\n",
        "\"Hello world\" â†’ [15496, 995]\n",
        "```\n",
        "\n",
        "But the transformer can't work with raw IDs! It needs **dense vectors** that capture meaning:\n",
        "\n",
        "```\n",
        "\"Hello world\" â†’ [15496, 995] â†’ [[0.12, -0.34, ...], [-0.05, 0.78, ...]] â†’ Transformer\n",
        "                 token IDs        768-dim embeddings (2 Ã— 768 matrix)\n",
        "```\n",
        "\n",
        "This is what embeddings do!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup: Load Model Weights and Tokenizer\n",
        "\n",
        "We'll reuse what we built in Steps 1 and 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import regex\n",
        "from functools import lru_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 160 weight tensors\n",
            "\n",
            "Key hyperparameters:\n",
            "  n_vocab: 50,257\n",
            "  n_embd:  768\n",
            "  n_ctx:   1024\n"
          ]
        }
      ],
      "source": [
        "# Load SafeTensors (from Step 1)\n",
        "def load_safetensors(path: str) -> dict:\n",
        "    \"\"\"Load a SafeTensors file without external library.\"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        header_size = int.from_bytes(f.read(8), 'little')\n",
        "        header_json = f.read(header_size).decode('utf-8')\n",
        "        header = json.loads(header_json)\n",
        "        header.pop(\"__metadata__\", {})\n",
        "        \n",
        "        tensors = {}\n",
        "        data_start = 8 + header_size\n",
        "        \n",
        "        for name, info in header.items():\n",
        "            dtype_map = {\"F32\": np.float32, \"F16\": np.float16, \"I64\": np.int64}\n",
        "            dtype = dtype_map.get(info[\"dtype\"], np.float32)\n",
        "            offset_start, offset_end = info[\"data_offsets\"]\n",
        "            \n",
        "            f.seek(data_start + offset_start)\n",
        "            raw_data = f.read(offset_end - offset_start)\n",
        "            tensors[name] = np.frombuffer(raw_data, dtype=dtype).reshape(info[\"shape\"])\n",
        "        \n",
        "        return tensors\n",
        "\n",
        "# Load model weights\n",
        "tensors = load_safetensors(\"models/gpt2/model.safetensors\")\n",
        "\n",
        "# Load config\n",
        "with open(\"models/gpt2/config.json\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(tensors)} weight tensors\")\n",
        "print(f\"\\nKey hyperparameters:\")\n",
        "print(f\"  n_vocab: {config['vocab_size']:,}\")\n",
        "print(f\"  n_embd:  {config['n_embd']}\")\n",
        "print(f\"  n_ctx:   {config['n_ctx']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Tokenizer loaded with 50,257 tokens\n"
          ]
        }
      ],
      "source": [
        "# GPT-2 Tokenizer (from Step 2)\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"Create byte-to-unicode mapping for GPT-2.\"\"\"\n",
        "    printable = list(range(ord(\"!\"), ord(\"~\") + 1))\n",
        "    printable += list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1))\n",
        "    printable += list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n",
        "    \n",
        "    byte_values = printable[:]\n",
        "    unicode_chars = printable[:]\n",
        "    \n",
        "    n = 0\n",
        "    for b in range(256):\n",
        "        if b not in printable:\n",
        "            byte_values.append(b)\n",
        "            unicode_chars.append(256 + n)\n",
        "            n += 1\n",
        "    \n",
        "    return {b: chr(c) for b, c in zip(byte_values, unicode_chars)}\n",
        "\n",
        "\n",
        "class GPT2Tokenizer:\n",
        "    \"\"\"BPE tokenizer for GPT-2 (from Step 2).\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_path=\"models/gpt2/vocab.json\", merges_path=\"models/gpt2/merges.txt\"):\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.vocab = json.load(f)\n",
        "        self.vocab_inverse = {v: k for k, v in self.vocab.items()}\n",
        "        \n",
        "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            merges_text = f.read()\n",
        "        merge_lines = [line for line in merges_text.split(\"\\n\")[1:] if line]\n",
        "        self.bpe_merges = {}\n",
        "        for i, line in enumerate(merge_lines):\n",
        "            parts = line.split(\" \")\n",
        "            if len(parts) == 2:\n",
        "                self.bpe_merges[tuple(parts)] = i\n",
        "        \n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.pattern = regex.compile(\n",
        "            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        )\n",
        "        self.cache = {}\n",
        "    \n",
        "    def _get_pairs(self, word):\n",
        "        return set((word[i], word[i + 1]) for i in range(len(word) - 1))\n",
        "    \n",
        "    def _bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        \n",
        "        word = list(token)\n",
        "        while len(word) > 1:\n",
        "            pairs = self._get_pairs(word)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = min(pairs, key=lambda p: self.bpe_merges.get(p, float('inf')))\n",
        "            if best_pair not in self.bpe_merges:\n",
        "                break\n",
        "            first, second = best_pair\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "        \n",
        "        result = tuple(word)\n",
        "        self.cache[token] = result\n",
        "        return result\n",
        "    \n",
        "    def encode(self, text):\n",
        "        token_ids = []\n",
        "        for chunk in self.pattern.findall(text):\n",
        "            byte_encoded = ''.join(self.byte_encoder[b] for b in chunk.encode('utf-8'))\n",
        "            bpe_tokens = self._bpe(byte_encoded)\n",
        "            for token in bpe_tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, token_ids):\n",
        "        text = ''.join(self.vocab_inverse[tid] for tid in token_ids)\n",
        "        byte_values = [self.byte_decoder[c] for c in text]\n",
        "        return bytes(byte_values).decode('utf-8', errors='replace')\n",
        "    \n",
        "    def get_token_string(self, token_id):\n",
        "        \"\"\"Get the string representation of a token ID.\"\"\"\n",
        "        return self.vocab_inverse.get(token_id, \"<UNK>\")\n",
        "\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = GPT2Tokenizer()\n",
        "print(f\"âœ… Tokenizer loaded with {len(tokenizer.vocab):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Why Can't We Just Use Token IDs?\n",
        "\n",
        "You might wonder: why not just feed the token IDs `[15496, 995]` directly into the neural network?\n",
        "\n",
        "### Problem 1: IDs Don't Capture Meaning\n",
        "\n",
        "Token IDs are **arbitrary numbers**. The ID assignment has no semantic meaning:\n",
        "\n",
        "```\n",
        "\"king\" â†’ ID 3364\n",
        "\"queen\" â†’ ID 21901\n",
        "\"banana\" â†’ ID 39897\n",
        "```\n",
        "\n",
        "Mathematically, `3364` is closer to `21901` than to `39897`, but that says nothing about meaning! We need a representation where **similar words are close together**.\n",
        "\n",
        "### Problem 2: Discrete vs. Continuous\n",
        "\n",
        "Neural networks work best with **continuous** values. Token IDs are discrete â€” there's no \"in between\" token 3364 and 3365. Embeddings give us a continuous space where:\n",
        "\n",
        "- Small changes in the vector â†’ small changes in meaning\n",
        "- We can do arithmetic (famous: `king - man + woman â‰ˆ queen`)\n",
        "- Gradients can flow smoothly during training\n",
        "\n",
        "### Problem 3: Dimensionality\n",
        "\n",
        "A single token ID is 1 number. The model needs **768 dimensions** of features to work with! Each dimension can capture different aspects of meaning:\n",
        "\n",
        "- Is this a noun or verb?\n",
        "- Is this formal or casual?\n",
        "- Is this about animals, food, technology?\n",
        "- Is this positive or negative?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Token Embeddings: `wte.weight`\n",
        "\n",
        "The **token embedding matrix** is simply a lookup table:\n",
        "\n",
        "```\n",
        "wte.weight: [50257, 768]\n",
        "            vocab size   embedding dim\n",
        "```\n",
        "\n",
        "Each of the 50,257 tokens has its own 768-dimensional vector. To embed a token, we just **look up its row**:\n",
        "\n",
        "```python\n",
        "token_id = 15496  # \"Hello\"\n",
        "embedding = wte.weight[token_id]  # 768-dim vector\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Embedding Matrix (wte.weight)\n",
            "  Shape: (50257, 768)\n",
            "  - 50,257 tokens (vocabulary size)\n",
            "  - 768 dimensions per token\n",
            "  Total parameters: 38,597,376\n",
            "  Memory: 154.4 MB\n"
          ]
        }
      ],
      "source": [
        "# Get the token embedding matrix\n",
        "wte = tensors[\"wte.weight\"]\n",
        "\n",
        "print(\"Token Embedding Matrix (wte.weight)\")\n",
        "print(f\"  Shape: {wte.shape}\")\n",
        "print(f\"  - {wte.shape[0]:,} tokens (vocabulary size)\")\n",
        "print(f\"  - {wte.shape[1]} dimensions per token\")\n",
        "print(f\"  Total parameters: {wte.size:,}\")\n",
        "print(f\"  Memory: {wte.nbytes / 1e6:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Hello' â†’ Token ID: 15496\n",
            "Embedding shape: (768,)\n",
            "\n",
            "First 20 values of the embedding:\n",
            "[-0.06865076 -0.1326938   0.0112033  -0.14665768 -0.1841705  -0.03579693\n",
            " -0.21733314 -0.17133704 -0.01038615 -0.01419895  0.00569266 -0.12377474\n",
            " -0.00720884  0.04176147 -0.03684433 -0.09803814  0.13315281  0.0523295\n",
            " -0.15724912  0.11166326]\n"
          ]
        }
      ],
      "source": [
        "def get_token_embedding(token_id):\n",
        "    \"\"\"\n",
        "    Look up the embedding for a token ID.\n",
        "    \n",
        "    This is the fundamental operation: token_id â†’ 768-dim vector\n",
        "    \"\"\"\n",
        "    return wte[token_id]\n",
        "\n",
        "\n",
        "# Let's embed the word \"Hello\"\n",
        "hello_ids = tokenizer.encode(\"Hello\")\n",
        "print(f\"'Hello' â†’ Token ID: {hello_ids[0]}\")\n",
        "\n",
        "embedding = get_token_embedding(hello_ids[0])\n",
        "print(f\"Embedding shape: {embedding.shape}\")\n",
        "print(f\"\\nFirst 20 values of the embedding:\")\n",
        "print(embedding[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Embeddings for Different Words\n",
            "======================================================================\n",
            "Word           Token ID First 5 values of embedding\n",
            "----------------------------------------------------------------------\n",
            "'Hello'       15496   [-0.069, -0.133, 0.011, -0.147, -0.184, ...]\n",
            "' the'         262   [-0.039, 0.005, 0.042, 0.040, -0.036, ...]\n",
            "' king'        5822   [-0.019, -0.117, 0.089, 0.073, 0.165, ...]\n",
            "' queen'       16599   [-0.020, -0.290, -0.032, -0.059, 0.052, ...]\n",
            "' man'         582   [0.004, -0.005, 0.010, -0.033, -0.054, ...]\n",
            "' woman'        2415   [0.040, -0.031, -0.028, -0.043, -0.148, ...]\n",
            "' dog'        3290   [0.094, -0.077, 0.035, -0.035, -0.089, ...]\n",
            "' cat'        3797   [0.010, 0.037, 0.164, -0.219, 0.029, ...]\n"
          ]
        }
      ],
      "source": [
        "# Let's look at embeddings for a few words\n",
        "words_to_embed = [\"Hello\", \" the\", \" king\", \" queen\", \" man\", \" woman\", \" dog\", \" cat\"]\n",
        "\n",
        "print(\"Token Embeddings for Different Words\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Word':<12} {'Token ID':>10} {'First 5 values of embedding'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for word in words_to_embed:\n",
        "    token_ids = tokenizer.encode(word)\n",
        "    if len(token_ids) == 1:  # Only show single-token words\n",
        "        token_id = token_ids[0]\n",
        "        emb = get_token_embedding(token_id)\n",
        "        values_str = \", \".join(f\"{v:.3f}\" for v in emb[:5])\n",
        "        print(f\"'{word}'  {token_id:>10}   [{values_str}, ...]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embedding a Full Sentence\n",
        "\n",
        "When we embed a sentence, we look up the embedding for **each token** and stack them:\n",
        "\n",
        "```\n",
        "\"Hello world\" â†’ [15496, 995] â†’ [[emb_0], [emb_1]] â†’ shape: (2, 768)\n",
        "```\n",
        "\n",
        "The result is a 2D matrix where:\n",
        "- Each **row** is a token's embedding\n",
        "- Each **column** is a feature dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'Hello world'\n",
            "Token IDs: [15496, 995]\n",
            "\n",
            "Token Embeddings shape: (2, 768)\n",
            "  - 2 tokens\n",
            "  - 768 dimensions each\n"
          ]
        }
      ],
      "source": [
        "def embed_tokens(token_ids):\n",
        "    \"\"\"\n",
        "    Embed a sequence of token IDs.\n",
        "    \n",
        "    Args:\n",
        "        token_ids: List of token IDs\n",
        "        \n",
        "    Returns:\n",
        "        numpy array of shape (seq_len, 768)\n",
        "    \"\"\"\n",
        "    # This is just a batch lookup: wte[token_ids]\n",
        "    return wte[token_ids]\n",
        "\n",
        "\n",
        "# Embed a sentence\n",
        "sentence = \"Hello world\"\n",
        "token_ids = tokenizer.encode(sentence)\n",
        "\n",
        "print(f\"Sentence: '{sentence}'\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# Get embeddings\n",
        "token_embeddings = embed_tokens(token_ids)\n",
        "\n",
        "print(f\"\\nToken Embeddings shape: {token_embeddings.shape}\")\n",
        "print(f\"  - {token_embeddings.shape[0]} tokens\")\n",
        "print(f\"  - {token_embeddings.shape[1]} dimensions each\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embedding matrix for 'Hello world':\n",
            "\n",
            "First 10 dimensions of each token's embedding:\n",
            "\n",
            "Token              Dim 0    Dim 1    Dim 2    Dim 3    Dim 4 ...\n",
            "----------------------------------------------------------------------\n",
            "Hello             -0.069   -0.133    0.011   -0.147   -0.184 ...\n",
            "âŽµworld            -0.149    0.152    0.006   -0.114   -0.026 ...\n"
          ]
        }
      ],
      "source": [
        "# Visualize the embedding matrix for the sentence\n",
        "print(f\"\\nEmbedding matrix for '{sentence}':\")\n",
        "print()\n",
        "\n",
        "# Show first 10 dimensions for each token\n",
        "print(\"First 10 dimensions of each token's embedding:\")\n",
        "print()\n",
        "print(f\"{'Token':<15} {'Dim 0':>8} {'Dim 1':>8} {'Dim 2':>8} {'Dim 3':>8} {'Dim 4':>8} ...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, tid in enumerate(token_ids):\n",
        "    token_str = tokenizer.get_token_string(tid).replace(\"Ä \", \"âŽµ\")\n",
        "    emb = token_embeddings[i]\n",
        "    values = \" \".join(f\"{v:>8.3f}\" for v in emb[:5])\n",
        "    print(f\"{token_str:<15} {values} ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Position Embeddings: `wpe.weight`\n",
        "\n",
        "Transformers process all tokens **in parallel** (unlike RNNs which process left-to-right). Without position information, the model can't tell the difference between:\n",
        "\n",
        "```\n",
        "\"The cat sat on the mat\"\n",
        "\"mat the on sat cat The\"\n",
        "```\n",
        "\n",
        "Both would produce the same set of token embeddings (just in a different order), but the model wouldn't know the order!\n",
        "\n",
        "### GPT-2's Solution: Learned Position Embeddings\n",
        "\n",
        "GPT-2 has a separate embedding table for **positions**:\n",
        "\n",
        "```\n",
        "wpe.weight: [1024, 768]\n",
        "            max positions  embedding dim\n",
        "```\n",
        "\n",
        "Each position (0, 1, 2, ..., 1023) has its own learned 768-dimensional vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position Embedding Matrix (wpe.weight)\n",
            "  Shape: (1024, 768)\n",
            "  - 1,024 positions (max context length)\n",
            "  - 768 dimensions per position\n",
            "  Total parameters: 786,432\n",
            "  Memory: 3.1 MB\n"
          ]
        }
      ],
      "source": [
        "# Get the position embedding matrix\n",
        "wpe = tensors[\"wpe.weight\"]\n",
        "\n",
        "print(\"Position Embedding Matrix (wpe.weight)\")\n",
        "print(f\"  Shape: {wpe.shape}\")\n",
        "print(f\"  - {wpe.shape[0]:,} positions (max context length)\")\n",
        "print(f\"  - {wpe.shape[1]} dimensions per position\")\n",
        "print(f\"  Total parameters: {wpe.size:,}\")\n",
        "print(f\"  Memory: {wpe.nbytes / 1e6:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position Embeddings Comparison\n",
            "============================================================\n",
            "Position    0: [-0.019, -0.197, 0.004, 0.011, 0.064, ...]\n",
            "Position    1: [0.024, -0.054, -0.095, -0.013, -0.010, ...]\n",
            "Position    2: [0.004, -0.085, 0.055, -0.005, -0.026, ...]\n",
            "Position  100: [-0.000, -0.016, -0.010, -0.002, 0.001, ...]\n",
            "Position  500: [-0.003, 0.004, -0.029, 0.004, 0.003, ...]\n",
            "Position 1023: [0.000, 0.003, -0.002, 0.003, -0.001, ...]\n"
          ]
        }
      ],
      "source": [
        "def get_position_embedding(position):\n",
        "    \"\"\"\n",
        "    Look up the embedding for a position.\n",
        "    \n",
        "    Args:\n",
        "        position: Integer position (0 to 1023)\n",
        "        \n",
        "    Returns:\n",
        "        768-dim position embedding vector\n",
        "    \"\"\"\n",
        "    if position >= wpe.shape[0]:\n",
        "        raise ValueError(f\"Position {position} exceeds max context length {wpe.shape[0]}\")\n",
        "    return wpe[position]\n",
        "\n",
        "\n",
        "# Compare position embeddings\n",
        "print(\"Position Embeddings Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "positions = [0, 1, 2, 100, 500, 1023]\n",
        "for pos in positions:\n",
        "    emb = get_position_embedding(pos)\n",
        "    values_str = \", \".join(f\"{v:.3f}\" for v in emb[:5])\n",
        "    print(f\"Position {pos:4}: [{values_str}, ...]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position Embeddings for sequence length 2:\n",
            "  Shape: (2, 768)\n"
          ]
        }
      ],
      "source": [
        "def embed_positions(seq_len):\n",
        "    \"\"\"\n",
        "    Get position embeddings for a sequence.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of the sequence\n",
        "        \n",
        "    Returns:\n",
        "        numpy array of shape (seq_len, 768)\n",
        "    \"\"\"\n",
        "    positions = np.arange(seq_len)\n",
        "    return wpe[positions]\n",
        "\n",
        "\n",
        "# Get position embeddings for our sentence\n",
        "seq_len = len(token_ids)  # From \"Hello world\"\n",
        "position_embeddings = embed_positions(seq_len)\n",
        "\n",
        "print(f\"Position Embeddings for sequence length {seq_len}:\")\n",
        "print(f\"  Shape: {position_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Why Learned Positions (vs. Fixed Sinusoidal)?\n",
        "\n",
        "The original Transformer paper (\"Attention Is All You Need\") used **fixed sinusoidal** position encodings:\n",
        "\n",
        "```\n",
        "PE(pos, 2i) = sin(pos / 10000^(2i/d))\n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n",
        "```\n",
        "\n",
        "GPT-2 instead uses **learned** position embeddings â€” the model discovers what position patterns are useful during training.\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| Sinusoidal | Generalizes to longer sequences, no extra parameters | Fixed patterns may not be optimal |\n",
        "| Learned | Model can learn optimal patterns | Limited to training context length |\n",
        "\n",
        "**Note:** Modern models like LLaMA, GPT-4 use **RoPE** (Rotary Position Embedding) which combines benefits of both!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Combining Token + Position Embeddings\n",
        "\n",
        "The input to the transformer is simply the **sum** of token and position embeddings:\n",
        "\n",
        "```\n",
        "input_embedding = token_embedding + position_embedding\n",
        "```\n",
        "\n",
        "This might seem strange â€” why addition? Why not concatenation?\n",
        "\n",
        "### Why Addition Works\n",
        "\n",
        "1. **Same dimensionality**: Both are 768-dim, so addition is natural\n",
        "2. **Information superposition**: The model learns to encode different info in different dimensions\n",
        "3. **Efficiency**: No extra parameters or dimension increase\n",
        "4. **It works empirically!**: The model learns to disentangle meaning from position\n",
        "\n",
        "Think of it like mixing colors â€” red + blue = purple, but you can still tell both were there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_input_embeddings(text):\n",
        "    \"\"\"\n",
        "    Get the full input embeddings for text.\n",
        "    \n",
        "    This is the first step in GPT-2's forward pass:\n",
        "    1. Tokenize text -> token IDs\n",
        "    2. Look up token embeddings\n",
        "    3. Look up position embeddings\n",
        "    4. Add them together\n",
        "    \n",
        "    Args:\n",
        "        text: Input string\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (input_embeddings, token_ids, tokens)\n",
        "    \"\"\"\n",
        "    # Step 1: Tokenize\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    seq_len = len(token_ids)\n",
        "    \n",
        "    # Step 2: Token embeddings (look up wte)\n",
        "    token_emb = wte[token_ids]  # Shape: (seq_len, 768)\n",
        "    \n",
        "    # Step 3: Position embeddings (look up wpe)\n",
        "    positions = np.arange(seq_len)\n",
        "    position_emb = wpe[positions]  # Shape: (seq_len, 768)\n",
        "    \n",
        "    # Step 4: Add them!\n",
        "    input_emb = token_emb + position_emb  # Shape: (seq_len, 768)\n",
        "    \n",
        "    # Get token strings for display\n",
        "    tokens = [tokenizer.get_token_string(tid) for tid in token_ids]\n",
        "    \n",
        "    return input_emb, token_ids, tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding the text: 'The quick brown fox'\n",
            "============================================================\n",
            "\n",
            "1. Tokenization:\n",
            "   Position 0: 'The' -> ID 464\n",
            "   Position 1: 'âŽµquick' -> ID 2068\n",
            "   Position 2: 'âŽµbrown' -> ID 7586\n",
            "   Position 3: 'âŽµfox' -> ID 21831\n",
            "\n",
            "2. Token Embeddings: shape (4, 768)\n",
            "3. Position Embeddings: shape (4, 768)\n",
            "4. Combined Input: shape (4, 768)\n",
            "\n",
            "âœ… Ready for the transformer! Input shape: (4, 768)\n"
          ]
        }
      ],
      "source": [
        "# Let's see the full embedding process\n",
        "text = \"The quick brown fox\"\n",
        "\n",
        "print(f\"Embedding the text: '{text}'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "input_emb, token_ids, tokens = get_input_embeddings(text)\n",
        "\n",
        "print(f\"\\n1. Tokenization:\")\n",
        "for i, (tid, tok) in enumerate(zip(token_ids, tokens)):\n",
        "    display = tok.replace(\"Ä \", \"âŽµ\")\n",
        "    print(f\"   Position {i}: '{display}' -> ID {tid}\")\n",
        "\n",
        "print(f\"\\n2. Token Embeddings: shape {wte[token_ids].shape}\")\n",
        "print(f\"3. Position Embeddings: shape {wpe[:len(token_ids)].shape}\")\n",
        "print(f\"4. Combined Input: shape {input_emb.shape}\")\n",
        "\n",
        "print(f\"\\nâœ… Ready for the transformer! Input shape: {input_emb.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "How embeddings combine (first 3 dimensions):\n",
            "===========================================================================\n",
            "Token                        Token Emb +                Pos Emb = Combined\n",
            "---------------------------------------------------------------------------\n",
            "The               [-0.07, -0.02, 0.06] +  [-0.02, -0.20, +0.00] = [-0.09, -0.22, 0.07]\n",
            "âŽµquick            [-0.01, -0.03, 0.20] +  [+0.02, -0.05, -0.09] = [0.02, -0.08, 0.10]\n",
            "âŽµbrown             [-0.05, 0.12, 0.08] +  [+0.00, -0.08, +0.05] = [-0.05, 0.03, 0.13]\n",
            "âŽµfox                [0.10, 0.00, 0.11] +  [-0.00, -0.07, +0.11] = [0.10, -0.07, 0.22]\n"
          ]
        }
      ],
      "source": [
        "# Let's visualize how the embeddings combine\n",
        "print(\"\\nHow embeddings combine (first 3 dimensions):\")\n",
        "print(\"=\" * 75)\n",
        "print(f\"{'Token':<12} {'Token Emb':>25} + {'Pos Emb':>22} = {'Combined'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for i, (tid, tok) in enumerate(zip(token_ids, tokens)):\n",
        "    tok_emb = wte[tid][:3]\n",
        "    pos_emb = wpe[i][:3]\n",
        "    combined = input_emb[i][:3]\n",
        "    \n",
        "    display = tok.replace(\"Ä \", \"âŽµ\")\n",
        "    tok_str = \"[\" + \", \".join(f\"{v:.2f}\" for v in tok_emb) + \"]\"\n",
        "    pos_str = \"[\" + \", \".join(f\"{v:+.2f}\" for v in pos_emb) + \"]\"\n",
        "    comb_str = \"[\" + \", \".join(f\"{v:.2f}\" for v in combined) + \"]\"\n",
        "    \n",
        "    print(f\"{display:<12} {tok_str:>25} + {pos_str:>22} = {comb_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Visualizing Embedding Space\n",
        "\n",
        "Embeddings live in a 768-dimensional space. We can't visualize 768 dimensions, but we can use **cosine similarity** to measure how close embeddings are.\n",
        "\n",
        "Let's explore:\n",
        "1. **Token similarity** - Similar words should be close together\n",
        "2. **Position patterns** - How position embeddings change across the sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Similarity using Cosine Similarity\n",
            "==================================================\n",
            "(1.0 = identical, 0.0 = unrelated, -1.0 = opposite)\n",
            "\n",
            "  king       <-> queen     : +0.657 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  king       <-> man       : +0.367 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  queen      <-> woman     : +0.451 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  dog        <-> cat       : +0.550 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  dog        <-> computer  : +0.303 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  happy      <-> sad       : +0.550 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  happy      <-> joyful    : +0.589 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  run        <-> running   : +0.630 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors.\n",
        "    \n",
        "    Cosine similarity measures the angle between vectors:\n",
        "    - 1.0 = identical direction (very similar)\n",
        "    - 0.0 = perpendicular (unrelated)\n",
        "    - -1.0 = opposite direction (opposite meaning)\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "\n",
        "def compare_words(word1, word2):\n",
        "    \"\"\"Compare the similarity of two words using their embeddings.\"\"\"\n",
        "    ids1 = tokenizer.encode(word1)\n",
        "    ids2 = tokenizer.encode(word2)\n",
        "    \n",
        "    # Only compare single-token words\n",
        "    if len(ids1) != 1 or len(ids2) != 1:\n",
        "        return None\n",
        "    \n",
        "    emb1 = wte[ids1[0]]\n",
        "    emb2 = wte[ids2[0]]\n",
        "    \n",
        "    return cosine_similarity(emb1, emb2)\n",
        "\n",
        "\n",
        "# Compare some word pairs\n",
        "print(\"Word Similarity using Cosine Similarity\")\n",
        "print(\"=\" * 50)\n",
        "print(\"(1.0 = identical, 0.0 = unrelated, -1.0 = opposite)\\n\")\n",
        "\n",
        "word_pairs = [\n",
        "    (\" king\", \" queen\"),\n",
        "    (\" king\", \" man\"),\n",
        "    (\" queen\", \" woman\"),\n",
        "    (\" dog\", \" cat\"),\n",
        "    (\" dog\", \" computer\"),\n",
        "    (\" happy\", \" sad\"),\n",
        "    (\" happy\", \" joyful\"),\n",
        "    (\" run\", \" running\"),\n",
        "]\n",
        "\n",
        "for w1, w2 in word_pairs:\n",
        "    sim = compare_words(w1, w2)\n",
        "    if sim is not None:\n",
        "        bar = \"â–ˆ\" * int(sim * 20) if sim > 0 else \"\"\n",
        "        print(f\"  {w1.strip():<10} <-> {w2.strip():<10}: {sim:+.3f} {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Famous Word Analogy Test\n",
        "\n",
        "One of the most fascinating properties of word embeddings is that they capture **semantic relationships** through vector arithmetic:\n",
        "\n",
        "```\n",
        "king - man + woman â‰ˆ queen\n",
        "```\n",
        "\n",
        "This works because the embedding space encodes:\n",
        "- `king - man` = \"royalty\" concept\n",
        "- Adding `woman` to \"royalty\" = `queen`\n",
        "\n",
        "Let's test this with GPT-2's embeddings!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_single_token_embedding(word):\n",
        "    \"\"\"Get embedding for a single-token word.\"\"\"\n",
        "    ids = tokenizer.encode(word)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"'{word}' is not a single token (got {len(ids)} tokens)\")\n",
        "    return wte[ids[0]]\n",
        "\n",
        "\n",
        "def find_closest_tokens(embedding, top_k=10, exclude_ids=None):\n",
        "    \"\"\"\n",
        "    Find the tokens with embeddings closest to the given embedding.\n",
        "    \n",
        "    Args:\n",
        "        embedding: Target 768-dim vector\n",
        "        top_k: Number of closest tokens to return\n",
        "        exclude_ids: Token IDs to exclude from results\n",
        "        \n",
        "    Returns:\n",
        "        List of (token_id, token_string, similarity) tuples\n",
        "    \"\"\"\n",
        "    exclude_ids = exclude_ids or []\n",
        "    \n",
        "    # Normalize the query embedding\n",
        "    embedding_norm = embedding / np.linalg.norm(embedding)\n",
        "    \n",
        "    # Normalize all token embeddings\n",
        "    wte_norms = np.linalg.norm(wte, axis=1, keepdims=True)\n",
        "    wte_normalized = wte / wte_norms\n",
        "    \n",
        "    # Compute similarities (dot product of normalized vectors = cosine similarity)\n",
        "    similarities = wte_normalized @ embedding_norm\n",
        "    \n",
        "    # Get top-k indices\n",
        "    top_indices = np.argsort(similarities)[::-1]  # Descending order\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        if idx not in exclude_ids:\n",
        "            token_str = tokenizer.get_token_string(idx)\n",
        "            results.append((idx, token_str, similarities[idx]))\n",
        "            if len(results) >= top_k:\n",
        "                break\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Analogy: king - man + woman = ?\n",
            "==================================================\n",
            "\n",
            "Top 10 closest tokens to (king - man + woman):\n",
            "--------------------------------------------------\n",
            "   1. 'âŽµqueen' (similarity: 0.7085) ðŸ‘‘\n",
            "   2. 'âŽµprincess' (similarity: 0.6046) \n",
            "   3. 'âŽµQueen' (similarity: 0.5964) ðŸ‘‘\n",
            "   4. 'âŽµkings' (similarity: 0.5932) \n",
            "   5. 'Queen' (similarity: 0.5720) ðŸ‘‘\n",
            "   6. 'âŽµKing' (similarity: 0.5390) \n",
            "   7. 'âŽµmonarch' (similarity: 0.5248) \n",
            "   8. 'âŽµqueens' (similarity: 0.5193) ðŸ‘‘\n",
            "   9. 'âŽµgoddess' (similarity: 0.5166) \n",
            "  10. 'âŽµruler' (similarity: 0.5065) \n"
          ]
        }
      ],
      "source": [
        "# Test: king - man + woman = ?\n",
        "print(\"Word Analogy: king - man + woman = ?\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    king = get_single_token_embedding(\" king\")\n",
        "    man = get_single_token_embedding(\" man\")\n",
        "    woman = get_single_token_embedding(\" woman\")\n",
        "    \n",
        "    # Compute: king - man + woman\n",
        "    result_embedding = king - man + woman\n",
        "    \n",
        "    # Find closest tokens (exclude the input words)\n",
        "    exclude = [tokenizer.encode(w)[0] for w in [\" king\", \" man\", \" woman\"]]\n",
        "    closest = find_closest_tokens(result_embedding, top_k=10, exclude_ids=exclude)\n",
        "    \n",
        "    print(\"\\nTop 10 closest tokens to (king - man + woman):\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, (tid, token_str, sim) in enumerate(closest, 1):\n",
        "        display = token_str.replace(\"Ä \", \"âŽµ\")\n",
        "        highlight = \"ðŸ‘‘\" if \"queen\" in token_str.lower() else \"\"\n",
        "        print(f\"  {i:2}. '{display}' (similarity: {sim:.4f}) {highlight}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "More Word Analogies\n",
            "============================================================\n",
            "\n",
            "Paris : France :: Tokyo : ?\n",
            "  Expected: Japan\n",
            "  Top 3 predictions:\n",
            "    'âŽµJapan' (0.723) âœ“\n",
            "    'Japan' (0.658) âœ“\n",
            "    'âŽµJapanese' (0.559) âœ“\n",
            "\n",
            "big : bigger :: small : ?\n",
            "  Expected: smaller\n",
            "  Top 3 predictions:\n",
            "    'âŽµsmaller' (0.797) âœ“\n",
            "    'âŽµlarger' (0.762) \n",
            "    'small' (0.608) \n",
            "\n",
            "walk : walked :: run : ?\n",
            "  Expected: ran\n",
            "  Top 3 predictions:\n",
            "    'âŽµran' (0.742) âœ“\n",
            "    'âŽµruns' (0.628) \n",
            "    'run' (0.549) \n"
          ]
        }
      ],
      "source": [
        "# Let's try more analogies!\n",
        "analogies = [\n",
        "    (\" Paris\", \" France\", \" Tokyo\", \"Japan\"),      # capital : country\n",
        "    (\" big\", \" bigger\", \" small\", \"smaller\"),     # comparative\n",
        "    (\" walk\", \" walked\", \" run\", \"ran\"),          # past tense\n",
        "]\n",
        "\n",
        "print(\"\\nMore Word Analogies\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for a, b, c, expected in analogies:\n",
        "    try:\n",
        "        emb_a = get_single_token_embedding(a)\n",
        "        emb_b = get_single_token_embedding(b)\n",
        "        emb_c = get_single_token_embedding(c)\n",
        "        \n",
        "        # a is to b as c is to ?\n",
        "        # result = b - a + c\n",
        "        result = emb_b - emb_a + emb_c\n",
        "        \n",
        "        exclude = [tokenizer.encode(w)[0] for w in [a, b, c]]\n",
        "        closest = find_closest_tokens(result, top_k=3, exclude_ids=exclude)\n",
        "        \n",
        "        print(f\"\\n{a.strip()} : {b.strip()} :: {c.strip()} : ?\")\n",
        "        print(f\"  Expected: {expected}\")\n",
        "        print(f\"  Top 3 predictions:\")\n",
        "        for tid, tok, sim in closest:\n",
        "            display = tok.replace(\"Ä \", \"âŽµ\")\n",
        "            match = \"âœ“\" if expected.lower() in tok.lower() else \"\"\n",
        "            print(f\"    '{display}' ({sim:.3f}) {match}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"\\n{a} : {b} :: {c} : ? - Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Position Embedding Analysis\n",
        "\n",
        "Let's analyze how position embeddings encode sequential information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position Embedding Similarity Matrix\n",
            "==================================================\n",
            "\n",
            "Comparing positions 0, 1, 2, 10, 100, 500, 1000:\n",
            "\n",
            "          0     1     2    10   100   500  1000\n",
            "   0   1.00  0.52  0.50  0.44  0.30  0.06 -0.22\n",
            "   1   0.52  1.00  0.92  0.72  0.31 -0.07 -0.41\n",
            "   2   0.50  0.92  1.00  0.83  0.34 -0.14 -0.47\n",
            "  10   0.44  0.72  0.83  1.00  0.25 -0.21 -0.61\n",
            " 100   0.30  0.31  0.34  0.25  1.00  0.31 -0.41\n",
            " 500   0.06 -0.07 -0.14 -0.21  0.31  1.00  0.32\n",
            "1000  -0.22 -0.41 -0.47 -0.61 -0.41  0.32  1.00\n"
          ]
        }
      ],
      "source": [
        "# Compare position embeddings\n",
        "print(\"Position Embedding Similarity Matrix\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nComparing positions 0, 1, 2, 10, 100, 500, 1000:\\n\")\n",
        "\n",
        "positions = [0, 1, 2, 10, 100, 500, 1000]\n",
        "pos_embeddings = [wpe[p] for p in positions]\n",
        "\n",
        "# Print header\n",
        "print(\"     \", end=\"\")\n",
        "for p in positions:\n",
        "    print(f\"{p:>6}\", end=\"\")\n",
        "print()\n",
        "\n",
        "# Compute and print similarity matrix\n",
        "for i, p1 in enumerate(positions):\n",
        "    print(f\"{p1:>4} \", end=\"\")\n",
        "    for j, p2 in enumerate(positions):\n",
        "        sim = cosine_similarity(pos_embeddings[i], pos_embeddings[j])\n",
        "        print(f\"{sim:>6.2f}\", end=\"\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Position Similarity vs Distance\n",
            "==================================================\n",
            "\n",
            "How similar is position 0 to positions at various distances?\n",
            "\n",
            "Distance from position 0:\n",
            "     1: +0.5242 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "     2: +0.5021 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "     5: +0.4890 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "    10: +0.4443 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "    20: +0.3869 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "    50: +0.3645 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "   100: +0.2991 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "   200: +0.1377 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "   500: +0.0612 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  1000: -0.2178 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
          ]
        }
      ],
      "source": [
        "# How does position similarity decay with distance?\n",
        "print(\"\\nPosition Similarity vs Distance\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nHow similar is position 0 to positions at various distances?\\n\")\n",
        "\n",
        "base_pos = 0\n",
        "base_emb = wpe[base_pos]\n",
        "\n",
        "distances = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
        "\n",
        "print(f\"Distance from position {base_pos}:\")\n",
        "for dist in distances:\n",
        "    if dist < wpe.shape[0]:\n",
        "        sim = cosine_similarity(base_emb, wpe[dist])\n",
        "        bar = \"â–ˆ\" * int((sim + 1) * 15)  # Shift to positive range for visualization\n",
        "        print(f\"  {dist:>4}: {sim:+.4f} {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Same Word, Different Position = Different Representation\n",
        "\n",
        "A key insight: the **same word at different positions** gets a **different final embedding**!\n",
        "\n",
        "```\n",
        "\"The cat and the dog\"  \n",
        "      ^           ^\n",
        "   \"the\" at      \"the\" at\n",
        "   position 0    position 3\n",
        "```\n",
        "\n",
        "Even though both are the word \"the\", they'll have different input embeddings because position embeddings are added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'I saw the dog and the cat'\n",
            "\n",
            "Tokens: ['I', 'âŽµsaw', 'âŽµthe', 'âŽµdog', 'âŽµand', 'âŽµthe', 'âŽµcat']\n",
            "Token IDs: [40, 2497, 262, 3290, 290, 262, 3797]\n",
            "\n",
            "============================================================\n",
            "Same token at different positions:\n",
            "============================================================\n",
            "\n",
            "Token 'âŽµthe' (ID 262) appears at positions: [2, 5]\n",
            "  Position 2:\n",
            "    Token emb[:3]:    [-0.039, 0.005, 0.042]\n",
            "    Position emb[:3]: [+0.004, -0.085, +0.055]\n",
            "    Combined[:3]:     [-0.035, -0.080, 0.097]\n",
            "  Position 5:\n",
            "    Token emb[:3]:    [-0.039, 0.005, 0.042]\n",
            "    Position emb[:3]: [+0.010, -0.034, +0.131]\n",
            "    Combined[:3]:     [-0.030, -0.029, 0.173]\n",
            "\n",
            "  Similarity between 'âŽµthe' at pos 2 vs 5: 0.9516\n",
            "  (Not 1.0 because different position embeddings are added!)\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate: same token, different positions\n",
        "# Note: \"The\" and \" the\" are DIFFERENT tokens (capitalization + space matters!)\n",
        "# We need a sentence where the SAME token appears twice, like \" the\" twice\n",
        "text = \"I saw the dog and the cat\"\n",
        "token_ids_demo = tokenizer.encode(text)\n",
        "tokens_demo = [tokenizer.get_token_string(tid) for tid in token_ids_demo]\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"\\nTokens: {[t.replace('Ä ', 'âŽµ') for t in tokens_demo]}\")\n",
        "print(f\"Token IDs: {token_ids_demo}\")\n",
        "\n",
        "# Find repeated tokens\n",
        "seen = {}\n",
        "for i, tid in enumerate(token_ids_demo):\n",
        "    if tid in seen:\n",
        "        seen[tid].append(i)\n",
        "    else:\n",
        "        seen[tid] = [i]\n",
        "\n",
        "# Find tokens that appear multiple times\n",
        "repeated = {tid: positions for tid, positions in seen.items() if len(positions) > 1}\n",
        "\n",
        "if repeated:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Same token at different positions:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for tid, positions in repeated.items():\n",
        "        token_str = tokenizer.get_token_string(tid).replace(\"Ä \", \"âŽµ\")\n",
        "        print(f\"\\nToken '{token_str}' (ID {tid}) appears at positions: {positions}\")\n",
        "        \n",
        "        # Get token embedding (same for both)\n",
        "        token_emb = wte[tid]\n",
        "        \n",
        "        # Get different position embeddings\n",
        "        for pos in positions:\n",
        "            pos_emb = wpe[pos]\n",
        "            combined = token_emb + pos_emb\n",
        "            print(f\"  Position {pos}:\")\n",
        "            print(f\"    Token emb[:3]:    [{', '.join(f'{v:.3f}' for v in token_emb[:3])}]\")\n",
        "            print(f\"    Position emb[:3]: [{', '.join(f'{v:+.3f}' for v in pos_emb[:3])}]\")\n",
        "            print(f\"    Combined[:3]:     [{', '.join(f'{v:.3f}' for v in combined[:3])}]\")\n",
        "        \n",
        "        # Compare the combined embeddings\n",
        "        emb1 = wte[tid] + wpe[positions[0]]\n",
        "        emb2 = wte[tid] + wpe[positions[1]]\n",
        "        sim = cosine_similarity(emb1, emb2)\n",
        "        print(f\"\\n  Similarity between '{token_str}' at pos {positions[0]} vs {positions[1]}: {sim:.4f}\")\n",
        "        print(f\"  (Not 1.0 because different position embeddings are added!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Complete Embedding Function\n",
        "\n",
        "Let's create a final, complete embedding function that we'll use in future notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2Embeddings(vocab_size=50257, n_embd=768, n_ctx=1024)\n"
          ]
        }
      ],
      "source": [
        "class GPT2Embeddings:\n",
        "    \"\"\"\n",
        "    GPT-2 Embedding layer.\n",
        "    \n",
        "    Combines token embeddings and position embeddings to create\n",
        "    the input representation for the transformer.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, wte, wpe, tokenizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            wte: Token embedding matrix [vocab_size, n_embd]\n",
        "            wpe: Position embedding matrix [n_ctx, n_embd]\n",
        "            tokenizer: GPT2Tokenizer instance\n",
        "        \"\"\"\n",
        "        self.wte = wte\n",
        "        self.wpe = wpe\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        self.vocab_size = wte.shape[0]\n",
        "        self.n_embd = wte.shape[1]\n",
        "        self.n_ctx = wpe.shape[0]\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        \"\"\"\n",
        "        Embed text into transformer input.\n",
        "        \n",
        "        Args:\n",
        "            text: Input string\n",
        "            \n",
        "        Returns:\n",
        "            dict with:\n",
        "                - embeddings: (seq_len, n_embd) input embeddings\n",
        "                - token_ids: list of token IDs\n",
        "                - tokens: list of token strings\n",
        "        \"\"\"\n",
        "        # Tokenize\n",
        "        token_ids = self.tokenizer.encode(text)\n",
        "        seq_len = len(token_ids)\n",
        "        \n",
        "        if seq_len > self.n_ctx:\n",
        "            raise ValueError(f\"Sequence length {seq_len} exceeds max context {self.n_ctx}\")\n",
        "        \n",
        "        # Look up embeddings\n",
        "        token_emb = self.wte[token_ids]\n",
        "        position_emb = self.wpe[:seq_len]\n",
        "        \n",
        "        # Combine\n",
        "        embeddings = token_emb + position_emb\n",
        "        \n",
        "        # Token strings for debugging\n",
        "        tokens = [self.tokenizer.get_token_string(tid) for tid in token_ids]\n",
        "        \n",
        "        return {\n",
        "            \"embeddings\": embeddings,\n",
        "            \"token_ids\": token_ids,\n",
        "            \"tokens\": tokens,\n",
        "            \"seq_len\": seq_len,\n",
        "        }\n",
        "    \n",
        "    def embed_ids(self, token_ids):\n",
        "        \"\"\"\n",
        "        Embed pre-tokenized IDs.\n",
        "        \n",
        "        Args:\n",
        "            token_ids: List or array of token IDs\n",
        "            \n",
        "        Returns:\n",
        "            (seq_len, n_embd) embeddings\n",
        "        \"\"\"\n",
        "        seq_len = len(token_ids)\n",
        "        token_emb = self.wte[token_ids]\n",
        "        position_emb = self.wpe[:seq_len]\n",
        "        return token_emb + position_emb\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return (f\"GPT2Embeddings(vocab_size={self.vocab_size}, \"\n",
        "                f\"n_embd={self.n_embd}, n_ctx={self.n_ctx})\")\n",
        "\n",
        "\n",
        "# Create our embedding layer\n",
        "embed_layer = GPT2Embeddings(wte, wpe, tokenizer)\n",
        "print(embed_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing GPT2Embeddings\n",
            "============================================================\n",
            "\n",
            "Text: 'Hello, world!'\n",
            "  Tokens: 4\n",
            "  Embedding shape: (4, 768)\n",
            "  Token breakdown: ['Hello', ',', 'âŽµworld', '!']...\n",
            "\n",
            "Text: 'The quick brown fox jumps over the lazy dog.'\n",
            "  Tokens: 10\n",
            "  Embedding shape: (10, 768)\n",
            "  Token breakdown: ['The', 'âŽµquick', 'âŽµbrown', 'âŽµfox', 'âŽµjumps']...\n",
            "\n",
            "Text: 'GPT-2 processes text using token and position embe...'\n",
            "  Tokens: 14\n",
            "  Embedding shape: (14, 768)\n",
            "  Token breakdown: ['G', 'PT', '-', '2', 'âŽµprocesses']...\n"
          ]
        }
      ],
      "source": [
        "# Test the complete embedding layer\n",
        "test_texts = [\n",
        "    \"Hello, world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"GPT-2 processes text using token and position embeddings.\",\n",
        "]\n",
        "\n",
        "print(\"Testing GPT2Embeddings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text in test_texts:\n",
        "    result = embed_layer(text)\n",
        "    print(f\"\\nText: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
        "    print(f\"  Tokens: {result['seq_len']}\")\n",
        "    print(f\"  Embedding shape: {result['embeddings'].shape}\")\n",
        "    print(f\"  Token breakdown: {[t.replace('Ä ', 'âŽµ') for t in result['tokens'][:5]]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embedding Statistics\n",
        "\n",
        "Let's look at some statistics of the embeddings to understand their properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Statistics\n",
            "============================================================\n",
            "\n",
            "Token Embeddings (wte):\n",
            "  Shape: (50257, 768)\n",
            "  Mean: 0.000380\n",
            "  Std:  0.143696\n",
            "  Min:  -1.269817\n",
            "  Max:  1.785156\n",
            "  Avg L2 norm: 3.9585\n",
            "\n",
            "Position Embeddings (wpe):\n",
            "  Shape: (1024, 768)\n",
            "  Mean: -0.000679\n",
            "  Std:  0.122691\n",
            "  Min:  -4.538114\n",
            "  Max:  4.065311\n",
            "  Avg L2 norm: 3.3901\n",
            "\n",
            "Relative magnitude:\n",
            "  Token embedding avg norm: 3.9585\n",
            "  Position embedding avg norm: 3.3901\n",
            "  Ratio (token/position): 1.17x\n"
          ]
        }
      ],
      "source": [
        "print(\"Embedding Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nToken Embeddings (wte):\")\n",
        "print(f\"  Shape: {wte.shape}\")\n",
        "print(f\"  Mean: {wte.mean():.6f}\")\n",
        "print(f\"  Std:  {wte.std():.6f}\")\n",
        "print(f\"  Min:  {wte.min():.6f}\")\n",
        "print(f\"  Max:  {wte.max():.6f}\")\n",
        "print(f\"  Avg L2 norm: {np.linalg.norm(wte, axis=1).mean():.4f}\")\n",
        "\n",
        "print(\"\\nPosition Embeddings (wpe):\")\n",
        "print(f\"  Shape: {wpe.shape}\")\n",
        "print(f\"  Mean: {wpe.mean():.6f}\")\n",
        "print(f\"  Std:  {wpe.std():.6f}\")\n",
        "print(f\"  Min:  {wpe.min():.6f}\")\n",
        "print(f\"  Max:  {wpe.max():.6f}\")\n",
        "print(f\"  Avg L2 norm: {np.linalg.norm(wpe, axis=1).mean():.4f}\")\n",
        "\n",
        "# Compare magnitudes\n",
        "token_norm = np.linalg.norm(wte, axis=1).mean()\n",
        "pos_norm = np.linalg.norm(wpe, axis=1).mean()\n",
        "print(f\"\\nRelative magnitude:\")\n",
        "print(f\"  Token embedding avg norm: {token_norm:.4f}\")\n",
        "print(f\"  Position embedding avg norm: {pos_norm:.4f}\")\n",
        "print(f\"  Ratio (token/position): {token_norm/pos_norm:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Does the Ratio Signify?\n",
        "\n",
        "The **ratio of token to position embedding norms** (~1.17x) tells us about the **relative importance of semantic meaning vs. positional information** in the combined embedding.\n",
        "\n",
        "Since the input to the transformer is simply:\n",
        "```\n",
        "input_embedding = token_embedding + position_embedding\n",
        "```\n",
        "\n",
        "If one were much larger than the other, it would **dominate** the combined signal, drowning out the smaller one.\n",
        "\n",
        "**Interpretation:**\n",
        "- **Ratio â‰ˆ 1**: Both sources of information contribute roughly equally\n",
        "- **Ratio >> 1**: Token identity would dominate over position\n",
        "- **Ratio << 1**: Position would dominate over token meaning\n",
        "\n",
        "**Why GPT-2's ~1.17x ratio is good:**\n",
        "\n",
        "This near-balance indicates GPT-2 learned to give roughly equal weight to:\n",
        "- **\"What is this word?\"** (semantic content from token embeddings)\n",
        "- **\"Where is this word?\"** (sequential position from position embeddings)\n",
        "\n",
        "This makes sense because both pieces of information are critical for language understanding â€” you need to know both *what* was said and *in what order* to understand meaning (e.g., \"dog bites man\" vs. \"man bites dog\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: What We Learned\n",
        "\n",
        "### The Embedding Pipeline\n",
        "\n",
        "```\n",
        "Input Text\n",
        "    |\n",
        "    v\n",
        "+----------------+\n",
        "|   Tokenizer    |  \"Hello world\" -> [15496, 995]\n",
        "+-------+--------+\n",
        "        |\n",
        "        v\n",
        "+----------------+\n",
        "|  Token Embed   |  wte[15496], wte[995] -> (2, 768)\n",
        "|  (wte lookup)  |\n",
        "+-------+--------+\n",
        "        |\n",
        "        v\n",
        "+----------------+\n",
        "| Position Emb   |  wpe[0], wpe[1] -> (2, 768)\n",
        "| (wpe lookup)   |\n",
        "+-------+--------+\n",
        "        |\n",
        "        v\n",
        "+----------------+\n",
        "|      Add       |  token_emb + position_emb -> (2, 768)\n",
        "+-------+--------+\n",
        "        |\n",
        "        v\n",
        "  Input to Transformer (2, 768)\n",
        "```\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Token Embeddings** (`wte.weight`)\n",
        "   - Shape: `[50257, 768]` - one row per vocabulary token\n",
        "   - Learned during training to capture semantic meaning\n",
        "   - Similar words have similar embeddings\n",
        "\n",
        "2. **Position Embeddings** (`wpe.weight`)\n",
        "   - Shape: `[1024, 768]` - one row per possible position\n",
        "   - Tell the model WHERE each token appears\n",
        "   - GPT-2 uses learned (not sinusoidal) positions\n",
        "\n",
        "3. **Combination**\n",
        "   - Simple addition: `input = token_emb + position_emb`\n",
        "   - Same word at different positions -> different representations\n",
        "\n",
        "4. **Embedding Properties**\n",
        "   - Cosine similarity captures semantic relationships\n",
        "   - Vector arithmetic works: `king - man + woman = queen`\n",
        "   - 768 dimensions encode diverse features\n",
        "\n",
        "### Next Step\n",
        "\n",
        "Now that we can convert text into meaningful vector representations, we're ready to learn about **attention** - the mechanism that allows the model to relate tokens to each other!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
