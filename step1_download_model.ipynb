{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Download GPT-2 Model Weights from HuggingFace\n",
        "\n",
        "In this notebook, we'll download GPT-2 in **SafeTensors format** and understand what's inside the model weights.\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. **What are Tensors?** - Multi-dimensional arrays that store model parameters\n",
        "2. **Token Embeddings** - How words become numbers\n",
        "3. **Position Embeddings** - How the model knows word order\n",
        "4. **Attention Heads** - How the model focuses on different parts of input\n",
        "5. **SafeTensors Format** - A simple, transparent file format for model weights\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Tensor?\n",
        "\n",
        "A **tensor** is essentially a multi-dimensional array of numbers. In machine learning:\n",
        "\n",
        "| Dimensions | Name | Example in GPT-2 |\n",
        "|------------|------|------------------|\n",
        "| 0D | Scalar | A single number |\n",
        "| 1D | Vector | Layer norm bias `[768]` |\n",
        "| 2D | Matrix | Token embeddings `[50257, 768]` |\n",
        "| 3D+ | Tensor | Batched inputs, attention weights, etc. |\n",
        "\n",
        "The model's **learned weights** are stored as tensors - millions of floating-point numbers that were optimized during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm  # Use notebook version for better display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "We'll download GPT-2 (124M parameters) from HuggingFace. The model consists of:\n",
        "- **model.safetensors**: The actual model weights (~548 MB)\n",
        "- **config.json**: Architecture hyperparameters (layers, heads, dimensions)\n",
        "- **vocab.json**: Vocabulary mapping (token string â†’ token ID)\n",
        "- **merges.txt**: BPE merge rules for tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace model repository\n",
        "MODEL_REPO = \"openai-community/gpt2\"\n",
        "HF_BASE_URL = f\"https://huggingface.co/{MODEL_REPO}/resolve/main\"\n",
        "\n",
        "# Files we need\n",
        "FILES_TO_DOWNLOAD = {\n",
        "    \"model.safetensors\": \"The model weights (548 MB)\",\n",
        "    \"config.json\": \"Model architecture config\",\n",
        "    \"vocab.json\": \"Vocabulary: token string -> token ID\",\n",
        "    \"merges.txt\": \"BPE merge rules for tokenization\",\n",
        "}\n",
        "\n",
        "MODELS_DIR = \"models/gpt2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file(url: str, dest: str, desc: str = None):\n",
        "    \"\"\"Download a file with progress bar.\"\"\"\n",
        "    response = requests.get(url, stream=True, allow_redirects=True)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    desc = desc or os.path.basename(dest)\n",
        "    \n",
        "    with open(dest, 'wb') as f:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "\n",
        "def download_gpt2(models_dir: str = MODELS_DIR) -> str:\n",
        "    \"\"\"Download GPT-2 files from HuggingFace.\"\"\"\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"ðŸ“¥ Downloading GPT-2 from HuggingFace ({MODEL_REPO})\\n\")\n",
        "    \n",
        "    for filename, description in FILES_TO_DOWNLOAD.items():\n",
        "        dest = os.path.join(models_dir, filename)\n",
        "        \n",
        "        if os.path.exists(dest):\n",
        "            print(f\"âœ“ {filename} already exists\")\n",
        "            continue\n",
        "        \n",
        "        url = f\"{HF_BASE_URL}/{filename}\"\n",
        "        print(f\"Downloading {filename} - {description}\")\n",
        "        download_file(url, dest, filename)\n",
        "        print()\n",
        "    \n",
        "    print(f\"âœ… All files downloaded to {models_dir}/\")\n",
        "    return models_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding SafeTensors Format\n",
        "\n",
        "SafeTensors is a simple, transparent file format for storing tensors. Unlike pickle-based formats, it's:\n",
        "- **Safe**: No arbitrary code execution\n",
        "- **Fast**: Memory-mapped loading\n",
        "- **Simple**: Easy to parse without external libraries!\n",
        "\n",
        "### File Structure\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Bytes 0-7: Header size (uint64, little-endian)               â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ Bytes 8 to 8+N: JSON header with tensor metadata             â”‚\n",
        "â”‚   {                                                          â”‚\n",
        "â”‚     \"tensor_name\": {                                         â”‚\n",
        "â”‚       \"dtype\": \"F32\",                                        â”‚\n",
        "â”‚       \"shape\": [768, 768],                                   â”‚\n",
        "â”‚       \"data_offsets\": [start, end]                           â”‚\n",
        "â”‚     },                                                       â”‚\n",
        "â”‚     ...                                                      â”‚\n",
        "â”‚   }                                                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ Rest: Raw tensor data (binary)                               â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_safetensors(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load a SafeTensors file WITHOUT the safetensors library!\n",
        "    \n",
        "    SafeTensors format is beautifully simple:\n",
        "    1. First 8 bytes: header size (uint64, little-endian)\n",
        "    2. Next N bytes: JSON header with tensor metadata\n",
        "    3. Rest: raw tensor data\n",
        "    \n",
        "    This is why SafeTensors is great for learning - it's transparent!\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ“¦ Loading SafeTensors from {path}\")\n",
        "    print(\"   (Parsing the raw binary format - no library needed!)\\n\")\n",
        "    \n",
        "    with open(path, 'rb') as f:\n",
        "        # Step 1: Read header size (first 8 bytes)\n",
        "        header_size = int.from_bytes(f.read(8), 'little')\n",
        "        print(f\"   Header size: {header_size} bytes\")\n",
        "        \n",
        "        # Step 2: Read and parse JSON header\n",
        "        header_json = f.read(header_size).decode('utf-8')\n",
        "        header = json.loads(header_json)\n",
        "        \n",
        "        # The header contains metadata and tensor info\n",
        "        # \"__metadata__\" key contains file-level metadata (optional)\n",
        "        metadata = header.pop(\"__metadata__\", {})\n",
        "        \n",
        "        print(f\"   Number of tensors: {len(header)}\")\n",
        "        print(f\"   Metadata: {metadata}\\n\")\n",
        "        \n",
        "        # Step 3: Load each tensor\n",
        "        tensors = {}\n",
        "        data_start = 8 + header_size  # Where tensor data begins\n",
        "        \n",
        "        for name, info in header.items():\n",
        "            dtype_str = info[\"dtype\"]\n",
        "            shape = info[\"shape\"]\n",
        "            offset_start, offset_end = info[\"data_offsets\"]\n",
        "            \n",
        "            # Map SafeTensors dtype to numpy dtype\n",
        "            dtype_map = {\n",
        "                \"F32\": np.float32,\n",
        "                \"F16\": np.float16,\n",
        "                \"BF16\": np.float16,  # We'll handle bfloat16 specially if needed\n",
        "                \"I64\": np.int64,\n",
        "                \"I32\": np.int32,\n",
        "            }\n",
        "            dtype = dtype_map.get(dtype_str, np.float32)\n",
        "            \n",
        "            # Read the raw bytes\n",
        "            f.seek(data_start + offset_start)\n",
        "            num_bytes = offset_end - offset_start\n",
        "            raw_data = f.read(num_bytes)\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            tensor = np.frombuffer(raw_data, dtype=dtype).reshape(shape)\n",
        "            tensors[name] = tensor\n",
        "        \n",
        "        return tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and Load the Model\n",
        "\n",
        "Let's download GPT-2 and load the weights!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Downloading GPT-2 from HuggingFace (openai-community/gpt2)\n",
            "\n",
            "âœ“ model.safetensors already exists\n",
            "âœ“ config.json already exists\n",
            "âœ“ vocab.json already exists\n",
            "âœ“ merges.txt already exists\n",
            "âœ… All files downloaded to models/gpt2/\n",
            "\n",
            "ðŸ“¦ Loading SafeTensors from models/gpt2/model.safetensors\n",
            "   (Parsing the raw binary format - no library needed!)\n",
            "\n",
            "   Header size: 14283 bytes\n",
            "   Number of tensors: 160\n",
            "   Metadata: {'format': 'pt'}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 160 tensors from the model!\n"
          ]
        }
      ],
      "source": [
        "# Download the model\n",
        "model_dir = download_gpt2()\n",
        "\n",
        "# Load the weights (parsing SafeTensors ourselves!)\n",
        "safetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
        "tensors = load_safetensors(safetensors_path)\n",
        "\n",
        "# Load the config\n",
        "config_path = os.path.join(model_dir, \"config.json\")\n",
        "with open(config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(f\"\\nLoaded {len(tensors)} tensors from the model!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding Token Embeddings (`wte.weight`)\n",
        "\n",
        "**Token embeddings** convert each token (word/subword) into a vector of numbers.\n",
        "\n",
        "In GPT-2, this is a **lookup table** with shape `(50257, 768)`:\n",
        "- **50257** = vocabulary size (all possible tokens)\n",
        "- **768** = embedding dimension\n",
        "\n",
        "### How it works:\n",
        "\n",
        "```\n",
        "\"Hello\" â†’ Token ID 15496 â†’ Look up row 15496 in wte.weight â†’ [0.12, -0.34, 0.56, ... 768 numbers]\n",
        "```\n",
        "\n",
        "Each token gets a unique 768-dimensional vector that captures its **semantic meaning**. Words with similar meanings end up with similar vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Embeddings (wte.weight)\n",
            "  Shape: (50257, 768)\n",
            "  - 50,257 tokens in vocabulary\n",
            "  - 768 dimensions per token\n",
            "  Total parameters: 38,597,376\n",
            "\n",
            "Example: First 10 values of token ID 0:\n",
            "  [-0.11010301 -0.03926672  0.03310751  0.13382645 -0.04847569 -0.07891767\n",
            " -0.23977417 -0.08947388  0.02525497 -0.10739683]\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the token embeddings\n",
        "wte = tensors[\"wte.weight\"]\n",
        "print(f\"Token Embeddings (wte.weight)\")\n",
        "print(f\"  Shape: {wte.shape}\")\n",
        "print(f\"  - {wte.shape[0]:,} tokens in vocabulary\")\n",
        "print(f\"  - {wte.shape[1]} dimensions per token\")\n",
        "print(f\"  Total parameters: {wte.size:,}\")\n",
        "print(f\"\\nExample: First 10 values of token ID 0:\")\n",
        "print(f\"  {wte[0, :10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Position Embeddings (`wpe.weight`)\n",
        "\n",
        "**Position embeddings** tell the model *where* each token appears in the sequence.\n",
        "\n",
        "In GPT-2, shape `(1024, 768)`:\n",
        "- **1024** = max sequence length (context window)\n",
        "- **768** = embedding dimension\n",
        "\n",
        "### Why position embeddings are needed:\n",
        "\n",
        "Transformers process all tokens in **parallel** (unlike RNNs which go step-by-step). Without position info:\n",
        "\n",
        "```\n",
        "\"The cat sat on the mat\"  â†â†’  \"mat the on sat cat The\"\n",
        "```\n",
        "\n",
        "These would look identical to the model! Position embeddings let the model know token order.\n",
        "\n",
        "### How Token + Position Embeddings Combine:\n",
        "\n",
        "```\n",
        "Input to GPT-2 = Token Embedding + Position Embedding\n",
        "```\n",
        "\n",
        "| Position | Token | Token Embedding | Position Embedding | Combined Input |\n",
        "|----------|-------|-----------------|-------------------|----------------|\n",
        "| 0 | \"Hello\" | `wte[15496]` | `wpe[0]` | `wte[15496] + wpe[0]` |\n",
        "| 1 | \"world\" | `wte[995]` | `wpe[1]` | `wte[995] + wpe[1]` |\n",
        "\n",
        "This gives each token a unique representation that encodes **both** its meaning **and** its position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position Embeddings (wpe.weight)\n",
            "  Shape: (1024, 768)\n",
            "  - 1,024 max positions (context window)\n",
            "  - 768 dimensions per position\n",
            "  Total parameters: 786,432\n",
            "\n",
            "Example: First 10 values of position 0:\n",
            "  [-0.01882072 -0.1974186   0.00402672  0.01134686  0.06382412 -0.10501328\n",
            "  0.03693705 -0.16802956 -0.04911101 -0.05646128]\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the position embeddings\n",
        "wpe = tensors[\"wpe.weight\"]\n",
        "print(f\"Position Embeddings (wpe.weight)\")\n",
        "print(f\"  Shape: {wpe.shape}\")\n",
        "print(f\"  - {wpe.shape[0]:,} max positions (context window)\")\n",
        "print(f\"  - {wpe.shape[1]} dimensions per position\")\n",
        "print(f\"  Total parameters: {wpe.size:,}\")\n",
        "print(f\"\\nExample: First 10 values of position 0:\")\n",
        "print(f\"  {wpe[0, :10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding Attention Heads\n",
        "\n",
        "An **attention head** is a mechanism that learns to **focus on different parts of the input** when processing each token. Think of it as one \"perspective\" the model uses to understand relationships between words.\n",
        "\n",
        "### Why Multiple Heads?\n",
        "\n",
        "GPT-2 uses **12 attention heads** (in the 124M model). Each head can learn to pay attention to **different patterns**:\n",
        "\n",
        "| Head | What it might learn |\n",
        "|------|---------------------|\n",
        "| Head 1 | Subject-verb relationships (\"The **cat** *sits*\") |\n",
        "| Head 2 | Adjective-noun pairs (\"**red** *apple*\") |\n",
        "| Head 3 | Pronouns to their referents (\"John... **he**\") |\n",
        "| Head 4 | Next word patterns |\n",
        "| ... | Different linguistic patterns |\n",
        "\n",
        "### How They Work Together\n",
        "\n",
        "```\n",
        "Input embedding (768 dimensions)\n",
        "         â”‚\n",
        "         â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚  Split  â”‚  768 Ã· 12 = 64 dims per head\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€ ... â”€â”€â”€â”\n",
        "    â–¼    â–¼    â–¼    â–¼    â–¼           â–¼\n",
        "  Head  Head  Head  Head  ...     Head\n",
        "   1     2     3     4             12\n",
        "  (64)  (64)  (64)  (64)          (64)\n",
        "    â”‚    â”‚    â”‚    â”‚               â”‚\n",
        "    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€ ... â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "         â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚ Concat  â”‚  â†’ Back to 768 dimensions\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### The Math\n",
        "\n",
        "```python\n",
        "n_embd = 768      # Total embedding dimension\n",
        "n_head = 12       # Number of attention heads\n",
        "head_dim = n_embd // n_head  # = 64 dimensions per head\n",
        "```\n",
        "\n",
        "Each head works with a **64-dimensional slice** of the full 768-dimensional space. The outputs from all 12 heads are concatenated back together.\n",
        "\n",
        "### Why This Design?\n",
        "\n",
        "**Multi-head attention** allows the model to:\n",
        "1. Learn **diverse patterns** (each head specializes differently)\n",
        "2. Process in **parallel** (all heads compute simultaneously)\n",
        "3. Capture **both local and global** relationships\n",
        "\n",
        "It's like having 12 different experts look at the same sentence, each focusing on different linguistic aspects, then combining their insights!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Head Configuration (from config.json)\n",
            "  n_head (number of heads):     12\n",
            "  n_embd (embedding dimension): 768\n",
            "  head_dim (per-head dim):      64 (= 768 Ã· 12)\n",
            "  n_layer (transformer blocks): 12\n",
            "\n",
            "Attention QKV projection weight (h.0.attn.c_attn.weight)\n",
            "  Shape: (768, 2304)\n",
            "  - Projects 768 â†’ 2304 (= 3 Ã— 768 for Q, K, V)\n"
          ]
        }
      ],
      "source": [
        "# Get attention head configuration from config.json\n",
        "n_head = config[\"n_head\"]\n",
        "n_embd = config[\"n_embd\"]\n",
        "n_layer = config[\"n_layer\"]\n",
        "head_dim = n_embd // n_head\n",
        "\n",
        "print(\"Attention Head Configuration (from config.json)\")\n",
        "print(f\"  n_head (number of heads):     {n_head}\")\n",
        "print(f\"  n_embd (embedding dimension): {n_embd}\")\n",
        "print(f\"  head_dim (per-head dim):      {head_dim} (= {n_embd} Ã· {n_head})\")\n",
        "print(f\"  n_layer (transformer blocks): {n_layer}\")\n",
        "\n",
        "# Show the attention weight that projects to Q, K, V\n",
        "c_attn = tensors[\"h.0.attn.c_attn.weight\"]\n",
        "print(f\"\\nAttention QKV projection weight (h.0.attn.c_attn.weight)\")\n",
        "print(f\"  Shape: {c_attn.shape}\")\n",
        "print(f\"  - Projects {c_attn.shape[0]} â†’ {c_attn.shape[1]} (= 3 Ã— {n_embd} for Q, K, V)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exploring All Model Weights\n",
        "\n",
        "Let's look at all the tensors in GPT-2 and understand what each one does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================================\n",
            "ðŸ” GPT-2 Weight Tensors\n",
            "===========================================================================\n",
            "\n",
            "ðŸ“Š EMBEDDINGS\n",
            "---------------------------------------------------------------------------\n",
            "  wpe.weight                               (1024, 768)               786,432 params\n",
            "  wte.weight                               (50257, 768)           38,597,376 params\n",
            "\n",
            "ðŸ“Š TRANSFORMER BLOCKS (showing block 0 as example)\n",
            "---------------------------------------------------------------------------\n",
            "  attn.bias                                (1, 1, 1024, 1024)      1,048,576 params\n",
            "  attn.c_attn.bias                         (2304,)                     2,304 params\n",
            "  attn.c_attn.weight                       (768, 2304)             1,769,472 params\n",
            "  attn.c_proj.bias                         (768,)                        768 params\n",
            "  attn.c_proj.weight                       (768, 768)                589,824 params\n",
            "  ln_1.bias                                (768,)                        768 params\n",
            "  ln_1.weight                              (768,)                        768 params\n",
            "  ln_2.bias                                (768,)                        768 params\n",
            "  ln_2.weight                              (768,)                        768 params\n",
            "  mlp.c_fc.bias                            (3072,)                     3,072 params\n",
            "  mlp.c_fc.weight                          (768, 3072)             2,359,296 params\n",
            "  mlp.c_proj.bias                          (768,)                        768 params\n",
            "  mlp.c_proj.weight                        (3072, 768)             2,359,296 params\n",
            "\n",
            "  ... (blocks 1-11 have the same structure)\n",
            "\n",
            "ðŸ“Š FINAL LAYER NORM\n",
            "---------------------------------------------------------------------------\n",
            "  ln_f.bias                                (768,)                        768 params\n",
            "  ln_f.weight                              (768,)                        768 params\n",
            "\n",
            "===========================================================================\n",
            "ðŸ“ˆ TOTAL PARAMETERS: 137,022,720\n",
            "===========================================================================\n"
          ]
        }
      ],
      "source": [
        "def explore_weights(tensors: dict):\n",
        "    \"\"\"Print information about all tensors in the model.\"\"\"\n",
        "    \n",
        "    print(\"=\" * 75)\n",
        "    print(\"ðŸ” GPT-2 Weight Tensors\")\n",
        "    print(\"=\" * 75)\n",
        "    \n",
        "    total_params = 0\n",
        "    \n",
        "    # Group by component\n",
        "    embeddings = {}\n",
        "    blocks = {}\n",
        "    final_ln = {}\n",
        "    \n",
        "    for name, tensor in sorted(tensors.items()):\n",
        "        params = tensor.size\n",
        "        total_params += params\n",
        "        \n",
        "        if name.startswith(\"wte\") or name.startswith(\"wpe\"):\n",
        "            embeddings[name] = tensor\n",
        "        elif name.startswith(\"h.\"):\n",
        "            # Extract block number\n",
        "            parts = name.split(\".\")\n",
        "            block_num = int(parts[1])\n",
        "            if block_num not in blocks:\n",
        "                blocks[block_num] = {}\n",
        "            blocks[block_num][name] = tensor\n",
        "        else:\n",
        "            final_ln[name] = tensor\n",
        "    \n",
        "    # Print embeddings\n",
        "    print(\"\\nðŸ“Š EMBEDDINGS\")\n",
        "    print(\"-\" * 75)\n",
        "    for name, tensor in embeddings.items():\n",
        "        print(f\"  {name:40} {str(tensor.shape):20} {tensor.size:>12,} params\")\n",
        "    \n",
        "    # Print one block in detail\n",
        "    print(\"\\nðŸ“Š TRANSFORMER BLOCKS (showing block 0 as example)\")\n",
        "    print(\"-\" * 75)\n",
        "    if 0 in blocks:\n",
        "        for name, tensor in sorted(blocks[0].items()):\n",
        "            short_name = name.replace(\"h.0.\", \"\")\n",
        "            print(f\"  {short_name:40} {str(tensor.shape):20} {tensor.size:>12,} params\")\n",
        "    \n",
        "    print(f\"\\n  ... (blocks 1-11 have the same structure)\")\n",
        "    \n",
        "    # Print final layer norm\n",
        "    print(\"\\nðŸ“Š FINAL LAYER NORM\")\n",
        "    print(\"-\" * 75)\n",
        "    for name, tensor in final_ln.items():\n",
        "        print(f\"  {name:40} {str(tensor.shape):20} {tensor.size:>12,} params\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 75)\n",
        "    print(f\"ðŸ“ˆ TOTAL PARAMETERS: {total_params:,}\")\n",
        "    print(\"=\" * 75)\n",
        "    \n",
        "    return total_params\n",
        "\n",
        "# Explore the weights\n",
        "total_params = explore_weights(tensors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GPT-2 Architecture Summary\n",
        "\n",
        "Now let's see the complete architecture with all hyperparameters and weight tensor explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===========================================================================\n",
            "ðŸ§  UNDERSTANDING THE ARCHITECTURE\n",
            "===========================================================================\n",
            "\n",
            "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "    â”‚                        GPT-2 Architecture                           â”‚\n",
            "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  HYPERPARAMETERS:                                                   â”‚\n",
            "    â”‚    â€¢ Vocabulary size (n_vocab):      50257                         â”‚\n",
            "    â”‚    â€¢ Max sequence length (n_ctx):     1024                         â”‚\n",
            "    â”‚    â€¢ Embedding dimension (n_embd):     768                         â”‚\n",
            "    â”‚    â€¢ Number of layers (n_layer):        12                         â”‚\n",
            "    â”‚    â€¢ Number of heads (n_head):          12                         â”‚\n",
            "    â”‚    â€¢ Head dimension:                    64                         â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  WEIGHT TENSORS EXPLAINED:                                          â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  Embeddings:                                                        â”‚\n",
            "    â”‚    wte.weight [50257, 768]                                     â”‚\n",
            "    â”‚      â””â”€ Token embeddings: each token ID â†’ 768-dim vector         â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    wpe.weight [1024, 768]                                      â”‚\n",
            "    â”‚      â””â”€ Position embeddings: each position â†’ 768-dim vector      â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  Per Transformer Block (Ã—12):                                     â”‚\n",
            "    â”‚    h.X.ln_1.weight/bias [768]                                     â”‚\n",
            "    â”‚      â””â”€ LayerNorm before attention                                  â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    h.X.attn.c_attn.weight [768, 2304]                          â”‚\n",
            "    â”‚      â””â”€ Combined Q, K, V projection (this is where Q,K,V come from!)â”‚\n",
            "    â”‚         Projects input â†’ [Q, K, V] concatenated                     â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    h.X.attn.c_proj.weight [768, 768]                            â”‚\n",
            "    â”‚      â””â”€ Output projection after attention                           â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    h.X.ln_2.weight/bias [768]                                     â”‚\n",
            "    â”‚      â””â”€ LayerNorm before MLP                                        â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    h.X.mlp.c_fc.weight [768, 3072]                             â”‚\n",
            "    â”‚      â””â”€ MLP first layer (expand to 4Ã— dimension)                    â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚    h.X.mlp.c_proj.weight [3072, 768]                           â”‚\n",
            "    â”‚      â””â”€ MLP second layer (project back to 768)                   â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  Final:                                                             â”‚\n",
            "    â”‚    ln_f.weight/bias [768]                                         â”‚\n",
            "    â”‚      â””â”€ Final LayerNorm before output                               â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â”‚  Note: GPT-2 ties wte.weight for input AND output (no separate lm_head)â”‚\n",
            "    â”‚                                                                     â”‚\n",
            "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "def explain_architecture(tensors: dict, config: dict):\n",
        "    \"\"\"Explain what each weight tensor does.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 75)\n",
        "    print(\"ðŸ§  UNDERSTANDING THE ARCHITECTURE\")\n",
        "    print(\"=\" * 75)\n",
        "    \n",
        "    # Get shapes for explanation\n",
        "    wte = tensors[\"wte.weight\"]\n",
        "    wpe = tensors[\"wpe.weight\"]\n",
        "    \n",
        "    n_vocab, n_embd = wte.shape\n",
        "    n_ctx, _ = wpe.shape\n",
        "    \n",
        "    # Count layers\n",
        "    n_layer = sum(1 for k in tensors if k.startswith(\"h.\") and \".attn.c_attn.weight\" in k)\n",
        "    \n",
        "    # Get number of attention heads from config\n",
        "    n_head = config[\"n_head\"]\n",
        "    head_dim = n_embd // n_head\n",
        "    \n",
        "    print(f\"\"\"\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚                        GPT-2 Architecture                           â”‚\n",
        "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  HYPERPARAMETERS:                                                   â”‚\n",
        "    â”‚    â€¢ Vocabulary size (n_vocab):     {n_vocab:>6}                         â”‚\n",
        "    â”‚    â€¢ Max sequence length (n_ctx):   {n_ctx:>6}                         â”‚\n",
        "    â”‚    â€¢ Embedding dimension (n_embd):  {n_embd:>6}                         â”‚\n",
        "    â”‚    â€¢ Number of layers (n_layer):    {n_layer:>6}                         â”‚\n",
        "    â”‚    â€¢ Number of heads (n_head):      {n_head:>6}                         â”‚\n",
        "    â”‚    â€¢ Head dimension:                {head_dim:>6}                         â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  WEIGHT TENSORS EXPLAINED:                                          â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  Embeddings:                                                        â”‚\n",
        "    â”‚    wte.weight [{n_vocab}, {n_embd}]                                     â”‚\n",
        "    â”‚      â””â”€ Token embeddings: each token ID â†’ {n_embd}-dim vector         â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    wpe.weight [{n_ctx}, {n_embd}]                                      â”‚\n",
        "    â”‚      â””â”€ Position embeddings: each position â†’ {n_embd}-dim vector      â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  Per Transformer Block (Ã—{n_layer}):                                     â”‚\n",
        "    â”‚    h.X.ln_1.weight/bias [{n_embd}]                                     â”‚\n",
        "    â”‚      â””â”€ LayerNorm before attention                                  â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    h.X.attn.c_attn.weight [{n_embd}, {3*n_embd}]                          â”‚\n",
        "    â”‚      â””â”€ Combined Q, K, V projection (this is where Q,K,V come from!)â”‚\n",
        "    â”‚         Projects input â†’ [Q, K, V] concatenated                     â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    h.X.attn.c_proj.weight [{n_embd}, {n_embd}]                            â”‚\n",
        "    â”‚      â””â”€ Output projection after attention                           â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    h.X.ln_2.weight/bias [{n_embd}]                                     â”‚\n",
        "    â”‚      â””â”€ LayerNorm before MLP                                        â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    h.X.mlp.c_fc.weight [{n_embd}, {4*n_embd}]                             â”‚\n",
        "    â”‚      â””â”€ MLP first layer (expand to 4Ã— dimension)                    â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚    h.X.mlp.c_proj.weight [{4*n_embd}, {n_embd}]                           â”‚\n",
        "    â”‚      â””â”€ MLP second layer (project back to {n_embd})                   â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  Final:                                                             â”‚\n",
        "    â”‚    ln_f.weight/bias [{n_embd}]                                         â”‚\n",
        "    â”‚      â””â”€ Final LayerNorm before output                               â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â”‚  Note: GPT-2 ties wte.weight for input AND output (no separate lm_head)â”‚\n",
        "    â”‚                                                                     â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    \"\"\")\n",
        "\n",
        "# Explain the architecture\n",
        "explain_architecture(tensors, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Step 1 Complete!\n",
        "\n",
        "We've successfully:\n",
        "1. Downloaded GPT-2 model weights from HuggingFace\n",
        "2. Parsed the SafeTensors format manually (no library needed!)\n",
        "3. Understood **tensors** as multi-dimensional arrays of learned parameters\n",
        "4. Explored **token embeddings** - how words become vectors\n",
        "5. Explored **position embeddings** - how the model knows word order\n",
        "6. Learned about **attention heads** - how the model focuses on different patterns\n",
        "\n",
        "### Next Step\n",
        "We'll implement the **tokenizer** to convert text â†’ tokens, so we can actually feed text into the model!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
