{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Build the Tokenizer from Scratch ðŸ”¤\n",
        "\n",
        "In this notebook, we'll implement **Byte Pair Encoding (BPE)** from scratch to understand how LLMs convert text into numbers.\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. **Why Tokenization?** - Why LLMs can't just use characters or words\n",
        "2. **Byte Pair Encoding** - The algorithm that powers GPT-2, GPT-3, GPT-4\n",
        "3. **Loading the Vocabulary** - Understanding `vocab.json` and `merges.txt`\n",
        "4. **Encoding** - Convert text â†’ token IDs\n",
        "5. **Decoding** - Convert token IDs â†’ text\n",
        "\n",
        "---\n",
        "\n",
        "## Why Do We Need Tokenization?\n",
        "\n",
        "Neural networks work with **numbers**, not text. We need to convert:\n",
        "\n",
        "```\n",
        "\"Hello world\" â†’ [15496, 995] â†’ Model â†’ [284, 262, ...] â†’ \"to the ...\"\n",
        "```\n",
        "\n",
        "### Option 1: Characters âŒ\n",
        "\n",
        "```\n",
        "\"Hello\" â†’ ['H', 'e', 'l', 'l', 'o'] â†’ [7, 4, 11, 11, 14]\n",
        "```\n",
        "\n",
        "**Problems:**\n",
        "- Very long sequences (each character = 1 token)\n",
        "- Model must learn spelling from scratch\n",
        "- Wastes capacity on low-level patterns\n",
        "\n",
        "### Option 2: Whole Words âŒ\n",
        "\n",
        "```\n",
        "\"tokenization\" â†’ ID 47832\n",
        "\"tokenize\" â†’ ID 12543\n",
        "\"tokens\" â†’ ID 8921\n",
        "```\n",
        "\n",
        "**Problems:**\n",
        "- Huge vocabulary (every word form needs its own ID)\n",
        "- Unknown words can't be handled\n",
        "- \"tokenization\" and \"tokenize\" share no representation!\n",
        "\n",
        "### Option 3: Subwords âœ… (BPE)\n",
        "\n",
        "```\n",
        "\"tokenization\" â†’ [\"token\", \"ization\"] â†’ [30001, 1634]\n",
        "\"tokenize\" â†’ [\"token\", \"ize\"] â†’ [30001, 1096]\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Compact vocabulary (GPT-2 uses 50,257 tokens)\n",
        "- Common words = single tokens\n",
        "- Rare words = broken into meaningful subwords\n",
        "- Shared subwords share learned representations!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## How BPE Works: The Core Idea\n",
        "\n",
        "BPE is elegantly simple:\n",
        "\n",
        "1. **Start** with all individual bytes (256 base tokens)\n",
        "2. **Count** which pairs of adjacent tokens appear most frequently\n",
        "3. **Merge** the most frequent pair into a new token\n",
        "4. **Repeat** until you reach your target vocabulary size\n",
        "\n",
        "### Training Example (How the vocabulary was created)\n",
        "\n",
        "Training corpus: `\"low lower lowest lowering\"`\n",
        "\n",
        "```\n",
        "Step 1: Start with characters\n",
        "  ['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ...]\n",
        "\n",
        "Step 2: Count pairs\n",
        "  ('l', 'o') appears 4 times  â† most frequent!\n",
        "  ('o', 'w') appears 4 times\n",
        "  ('e', 'r') appears 3 times\n",
        "  ...\n",
        "\n",
        "Step 3: Merge ('l', 'o') â†’ 'lo'\n",
        "  ['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ...]\n",
        "\n",
        "Step 4: Count again, merge again...\n",
        "  ('lo', 'w') â†’ 'low'\n",
        "  ['low', ' ', 'low', 'e', 'r', ...]\n",
        "\n",
        "Continue until vocabulary size reached!\n",
        "```\n",
        "\n",
        "The **merge rules** are saved to `merges.txt` during training. At inference time, we just apply these rules!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import regex  # Note: 'regex' not 're' - needed for Unicode categories\n",
        "from functools import lru_cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding GPT-2's Byte Encoding\n",
        "\n",
        "GPT-2 uses a clever trick: instead of working with raw bytes (0-255), it maps bytes to **printable Unicode characters**. This makes the vocabulary human-readable.\n",
        "\n",
        "### The Byte-to-Character Mapping\n",
        "\n",
        "```\n",
        "Byte 32 (space)    â†’ 'Ä ' (special character for space)\n",
        "Byte 33 ('!')      â†’ '!'\n",
        "Byte 65 ('A')      â†’ 'A'\n",
        "Byte 10 (newline)  â†’ 'ÄŠ' (special character)\n",
        "```\n",
        "\n",
        "The `Ä ` character you see in `merges.txt` represents a space at the start of a word. This is how GPT-2 distinguishes:\n",
        "- `\"the\"` at the start of text (no Ä )\n",
        "- `\" the\"` with a leading space (becomes `\"Ä the\"`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Byte to Unicode Mapping Examples:\n",
            "  Space (byte 32)   â†’ 'Ä ' (Ä  represents leading space)\n",
            "  'A' (byte 65)     â†’ 'A'\n",
            "  'a' (byte 97)     â†’ 'a'\n",
            "  Newline (byte 10) â†’ 'ÄŠ' (ÄŠ represents newline)\n",
            "  Tab (byte 9)      â†’ 'Ä‰'\n",
            "\n",
            "Total mappings: 256\n"
          ]
        }
      ],
      "source": [
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Create a mapping from bytes (0-255) to Unicode characters.\n",
        "    \n",
        "    GPT-2 uses this to make the vocabulary human-readable.\n",
        "    Non-printable bytes get mapped to unused Unicode characters.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from byte value (int) to Unicode character (str)\n",
        "    \"\"\"\n",
        "    # Printable ASCII characters (these stay the same)\n",
        "    # Ranges: ! to ~ (33-126), plus some extended Latin\n",
        "    printable = list(range(ord(\"!\"), ord(\"~\") + 1))  # 33-126\n",
        "    printable += list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1))  # 161-172\n",
        "    printable += list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))  # 174-255\n",
        "    \n",
        "    # Start with printable bytes mapping to themselves\n",
        "    byte_values = printable[:]\n",
        "    unicode_chars = printable[:]\n",
        "    \n",
        "    # For non-printable bytes (0-32, 127-160, 173), \n",
        "    # map them to Unicode characters starting at 256\n",
        "    n = 0\n",
        "    for b in range(256):\n",
        "        if b not in printable:\n",
        "            byte_values.append(b)\n",
        "            unicode_chars.append(256 + n)  # Use chars above Latin-1\n",
        "            n += 1\n",
        "    \n",
        "    # Convert to dict: byte_value -> Unicode character\n",
        "    return {b: chr(c) for b, c in zip(byte_values, unicode_chars)}\n",
        "\n",
        "\n",
        "# Create the mapping\n",
        "byte_encoder = bytes_to_unicode()\n",
        "byte_decoder = {v: k for k, v in byte_encoder.items()}  # Reverse mapping\n",
        "\n",
        "print(\"Byte to Unicode Mapping Examples:\")\n",
        "print(f\"  Space (byte 32)   â†’ '{byte_encoder[32]}' (Ä  represents leading space)\")\n",
        "print(f\"  'A' (byte 65)     â†’ '{byte_encoder[65]}'\")\n",
        "print(f\"  'a' (byte 97)     â†’ '{byte_encoder[97]}'\")\n",
        "print(f\"  Newline (byte 10) â†’ '{byte_encoder[10]}' (ÄŠ represents newline)\")\n",
        "print(f\"  Tab (byte 9)      â†’ '{byte_encoder[9]}'\")\n",
        "print(f\"\\nTotal mappings: {len(byte_encoder)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Loading the Vocabulary Files\n",
        "\n",
        "GPT-2's tokenizer needs two files:\n",
        "\n",
        "1. **`vocab.json`**: Maps token strings to token IDs\n",
        "2. **`merges.txt`**: Lists the BPE merge rules in order (earlier = higher priority)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50,257 tokens\n",
            "\n",
            "Some example tokens:\n",
            "  '!' â†’ 0\n",
            "  'hello' â†’ 31373\n",
            "  'Ä hello' â†’ 23748\n",
            "  'Ä the' â†’ 262\n",
            "  'Ä Hello' â†’ 18435\n"
          ]
        }
      ],
      "source": [
        "# Load vocabulary (token string -> token ID)\n",
        "with open(\"models/gpt2/vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Create reverse mapping (token ID -> token string)\n",
        "vocab_inverse = {v: k for k, v in vocab.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab):,} tokens\")\n",
        "print(f\"\\nSome example tokens:\")\n",
        "print(f\"  '!' â†’ {vocab['!']}\")\n",
        "print(f\"  'hello' â†’ {vocab.get('hello', 'NOT FOUND')}\")\n",
        "print(f\"  'Ä hello' â†’ {vocab.get('Ä hello', 'NOT FOUND')}\")\n",
        "print(f\"  'Ä the' â†’ {vocab['Ä the']}\")\n",
        "print(f\"  'Ä Hello' â†’ {vocab.get('Ä Hello', 'NOT FOUND')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of merge rules: 50,000\n",
            "\n",
            "First 10 merge rules (highest priority):\n",
            "  0: 'Ä ' + 't' â†’ 'Ä t'\n",
            "  1: 'Ä ' + 'a' â†’ 'Ä a'\n",
            "  2: 'h' + 'e' â†’ 'he'\n",
            "  3: 'i' + 'n' â†’ 'in'\n",
            "  4: 'r' + 'e' â†’ 're'\n",
            "  5: 'o' + 'n' â†’ 'on'\n",
            "  6: 'Ä t' + 'he' â†’ 'Ä the'\n",
            "  7: 'e' + 'r' â†’ 'er'\n",
            "  8: 'Ä ' + 's' â†’ 'Ä s'\n",
            "  9: 'a' + 't' â†’ 'at'\n"
          ]
        }
      ],
      "source": [
        "# Load BPE merge rules\n",
        "with open(\"models/gpt2/merges.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    merges_text = f.read()\n",
        "\n",
        "# Parse the merges file\n",
        "# First line is version info, skip it\n",
        "merge_lines = merges_text.split(\"\\n\")[1:]  # Skip \"#version: 0.2\"\n",
        "merge_lines = [line for line in merge_lines if line]  # Remove empty lines\n",
        "\n",
        "# Create merge rules: (token1, token2) -> priority (lower = merge first)\n",
        "bpe_merges = {}\n",
        "for i, line in enumerate(merge_lines):\n",
        "    parts = line.split(\" \")\n",
        "    if len(parts) == 2:\n",
        "        bpe_merges[tuple(parts)] = i\n",
        "\n",
        "print(f\"Number of merge rules: {len(bpe_merges):,}\")\n",
        "print(f\"\\nFirst 10 merge rules (highest priority):\")\n",
        "for i, line in enumerate(merge_lines[:10]):\n",
        "    a, b = line.split(\" \")\n",
        "    merged = a + b\n",
        "    print(f\"  {i}: '{a}' + '{b}' â†’ '{merged}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding the Merge Rules\n",
        "\n",
        "Let's decode what these merge rules mean:\n",
        "\n",
        "| Rule | Token 1 | Token 2 | Result | Meaning |\n",
        "|------|---------|---------|--------|---------|\n",
        "| 0 | `Ä ` | `t` | `Ä t` | Space + t |\n",
        "| 1 | `Ä ` | `a` | `Ä a` | Space + a |\n",
        "| 2 | `h` | `e` | `he` | \"he\" is common! |\n",
        "| 3 | `i` | `n` | `in` | \"in\" is common! |\n",
        "| 7 | `Ä t` | `he` | `Ä the` | Space + \"the\" |\n",
        "\n",
        "The order matters! Lower number = higher priority = gets merged first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some interesting merge rules:\n",
            "\n",
            "  Priority     6: 'Ä t' + 'he' â†’ 'Ä the' (displayed: 'âŽµthe')\n",
            "  Priority    30: 'Ä o' + 'f' â†’ 'Ä of' (displayed: 'âŽµof')\n"
          ]
        }
      ],
      "source": [
        "# Let's look at some interesting merge rules\n",
        "print(\"Some interesting merge rules:\")\n",
        "print()\n",
        "\n",
        "# Find merges that create common words\n",
        "interesting_merges = [\n",
        "    (\"Ä t\", \"he\"),   # â†’ \"Ä the\" (\" the\")\n",
        "    (\"Ä a\", \"nd\"),   # â†’ \"Ä and\" (\" and\")\n",
        "    (\"Ä o\", \"f\"),    # â†’ \"Ä of\" (\" of\")\n",
        "    (\"Ä i\", \"s\"),    # â†’ \"Ä is\" (\" is\")\n",
        "]\n",
        "\n",
        "for a, b in interesting_merges:\n",
        "    if (a, b) in bpe_merges:\n",
        "        priority = bpe_merges[(a, b)]\n",
        "        merged = a + b\n",
        "        # Decode for display\n",
        "        display = merged.replace(\"Ä \", \"âŽµ\")  # Show space as visible char\n",
        "        print(f\"  Priority {priority:5}: '{a}' + '{b}' â†’ '{merged}' (displayed: '{display}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The BPE Algorithm\n",
        "\n",
        "Now let's implement BPE! The algorithm has these steps:\n",
        "\n",
        "1. **Convert text to bytes** (using our byte encoder)\n",
        "2. **Split into initial tokens** (each byte is one token)\n",
        "3. **Repeatedly merge** the highest-priority pair\n",
        "4. **Stop** when no more merges apply\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['h', 'e', 'l', 'l', 'o']\n",
            "Pairs: {('e', 'l'), ('l', 'o'), ('h', 'e'), ('l', 'l')}\n"
          ]
        }
      ],
      "source": [
        "def get_pairs(tokens):\n",
        "    \"\"\"\n",
        "    Get all adjacent pairs from a list of tokens.\n",
        "    \n",
        "    Example:\n",
        "        ['h', 'e', 'l', 'l', 'o'] â†’ {('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')}\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    for i in range(len(tokens) - 1):\n",
        "        pairs.add((tokens[i], tokens[i + 1]))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Test it\n",
        "test_tokens = list(\"hello\")\n",
        "print(f\"Tokens: {test_tokens}\")\n",
        "print(f\"Pairs: {get_pairs(test_tokens)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bpe(token, verbose=False):\n",
        "    \"\"\"\n",
        "    Apply BPE merges to a single word/token.\n",
        "    \n",
        "    Args:\n",
        "        token: A string (already converted to byte-encoded characters)\n",
        "        verbose: If True, print each merge step\n",
        "        \n",
        "    Returns:\n",
        "        tuple: The token split into BPE subwords\n",
        "    \"\"\"\n",
        "    if len(token) <= 1:\n",
        "        return tuple(token)\n",
        "    \n",
        "    # Start with each character as a separate token\n",
        "    word = list(token)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Starting with: {word}\")\n",
        "    \n",
        "    while True:\n",
        "        # Get all adjacent pairs in current tokenization\n",
        "        pairs = get_pairs(word)\n",
        "        \n",
        "        if not pairs:\n",
        "            break\n",
        "        \n",
        "        # Find the pair with lowest merge priority (= should merge first)\n",
        "        # If a pair isn't in our merge rules, give it infinity priority\n",
        "        best_pair = min(pairs, key=lambda p: bpe_merges.get(p, float('inf')))\n",
        "        \n",
        "        # If best pair isn't in our merge rules, we're done\n",
        "        if best_pair not in bpe_merges:\n",
        "            break\n",
        "        \n",
        "        # Merge this pair everywhere in the word\n",
        "        first, second = best_pair\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            # Look for the pair to merge\n",
        "            if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
        "                # Merge!\n",
        "                new_word.append(first + second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        \n",
        "        if verbose:\n",
        "            priority = bpe_merges[best_pair]\n",
        "            print(f\"  Merge '{first}' + '{second}' (priority {priority}) â†’ {new_word}\")\n",
        "        \n",
        "        word = new_word\n",
        "        \n",
        "        # If we're down to a single token, we're done\n",
        "        if len(word) == 1:\n",
        "            break\n",
        "    \n",
        "    return tuple(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: ' the'\n",
            "Byte-encoded: 'Ä the'\n",
            "  (The Ä  character represents the leading space)\n",
            "\n",
            "Starting with: ['Ä ', 't', 'h', 'e']\n",
            "  Merge 'Ä ' + 't' (priority 0) â†’ ['Ä t', 'h', 'e']\n",
            "  Merge 'h' + 'e' (priority 2) â†’ ['Ä t', 'he']\n",
            "  Merge 'Ä t' + 'he' (priority 6) â†’ ['Ä the']\n",
            "\n",
            "Final BPE tokens: ('Ä the',)\n"
          ]
        }
      ],
      "source": [
        "# Let's test BPE on a simple word!\n",
        "# First, convert \"the\" to byte-encoded form\n",
        "def text_to_bytes(text):\n",
        "    \"\"\"Convert text to GPT-2's byte-encoded string.\"\"\"\n",
        "    return ''.join(byte_encoder[b] for b in text.encode('utf-8'))\n",
        "\n",
        "# Test on \" the\" (note the leading space!)\n",
        "test_word = \" the\"\n",
        "byte_encoded = text_to_bytes(test_word)\n",
        "\n",
        "print(f\"Original text: '{test_word}'\")\n",
        "print(f\"Byte-encoded: '{byte_encoded}'\")\n",
        "print(f\"  (The Ä  character represents the leading space)\")\n",
        "print()\n",
        "\n",
        "# Apply BPE with verbose output\n",
        "result = bpe(byte_encoded, verbose=True)\n",
        "print(f\"\\nFinal BPE tokens: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: ' tokenization'\n",
            "Byte-encoded: 'Ä tokenization'\n",
            "\n",
            "Starting with: ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n",
            "  Merge 'Ä ' + 't' (priority 0) â†’ ['Ä t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n",
            "  Merge 'o' + 'n' (priority 5) â†’ ['Ä t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'on']\n",
            "  Merge 'a' + 't' (priority 9) â†’ ['Ä t', 'o', 'k', 'e', 'n', 'i', 'z', 'at', 'i', 'on']\n",
            "  Merge 'e' + 'n' (priority 12) â†’ ['Ä t', 'o', 'k', 'en', 'i', 'z', 'at', 'i', 'on']\n",
            "  Merge 'Ä t' + 'o' (priority 28) â†’ ['Ä to', 'k', 'en', 'i', 'z', 'at', 'i', 'on']\n",
            "  Merge 'i' + 'on' (priority 39) â†’ ['Ä to', 'k', 'en', 'i', 'z', 'at', 'ion']\n",
            "  Merge 'at' + 'ion' (priority 85) â†’ ['Ä to', 'k', 'en', 'i', 'z', 'ation']\n",
            "  Merge 'i' + 'z' (priority 272) â†’ ['Ä to', 'k', 'en', 'iz', 'ation']\n",
            "  Merge 'iz' + 'ation' (priority 1378) â†’ ['Ä to', 'k', 'en', 'ization']\n",
            "  Merge 'k' + 'en' (priority 3208) â†’ ['Ä to', 'ken', 'ization']\n",
            "  Merge 'Ä to' + 'ken' (priority 10985) â†’ ['Ä token', 'ization']\n",
            "\n",
            "Final BPE tokens: ('Ä token', 'ization')\n"
          ]
        }
      ],
      "source": [
        "# Let's try a longer word: \"tokenization\"\n",
        "test_word = \" tokenization\"\n",
        "byte_encoded = text_to_bytes(test_word)\n",
        "\n",
        "print(f\"Original text: '{test_word}'\")\n",
        "print(f\"Byte-encoded: '{byte_encoded}'\")\n",
        "print()\n",
        "\n",
        "result = bpe(byte_encoded, verbose=True)\n",
        "print(f\"\\nFinal BPE tokens: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Pre-Tokenization: Splitting Text into Words\n",
        "\n",
        "Before applying BPE, GPT-2 first splits text into \"words\" using a regex pattern. This ensures:\n",
        "- Punctuation is separated from words\n",
        "- Numbers are treated specially\n",
        "- Contractions like \"don't\" stay together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'Hello, world! I'm learning about GPT-2's tokenizer. It uses 50257 tokens.'\n",
            "\n",
            "Pre-tokenized chunks (19 chunks):\n",
            "   0: 'Hello'\n",
            "   1: ','\n",
            "   2: 'âŽµworld'\n",
            "   3: '!'\n",
            "   4: 'âŽµI'\n",
            "   5: ''m'\n",
            "   6: 'âŽµlearning'\n",
            "   7: 'âŽµabout'\n",
            "   8: 'âŽµGPT'\n",
            "   9: '-'\n",
            "  10: '2'\n",
            "  11: ''s'\n",
            "  12: 'âŽµtokenizer'\n",
            "  13: '.'\n",
            "  14: 'âŽµIt'\n",
            "  15: 'âŽµuses'\n",
            "  16: 'âŽµ50257'\n",
            "  17: 'âŽµtokens'\n",
            "  18: '.'\n"
          ]
        }
      ],
      "source": [
        "# GPT-2's pre-tokenization pattern\n",
        "# This splits text into chunks before BPE is applied to each chunk\n",
        "GPT2_PATTERN = regex.compile(\n",
        "    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        ")\n",
        "\n",
        "# Let's understand this pattern:\n",
        "# - 's|'t|'re|'ve|'m|'ll|'d  : English contractions\n",
        "# - | ?\\p{L}+                : Optional space + letters (words)\n",
        "# - | ?\\p{N}+                : Optional space + numbers  \n",
        "# - | ?[^\\s\\p{L}\\p{N}]+      : Optional space + punctuation\n",
        "# - |\\s+(?!\\S)               : Trailing whitespace\n",
        "# - |\\s+                     : Other whitespace\n",
        "\n",
        "test_text = \"Hello, world! I'm learning about GPT-2's tokenizer. It uses 50257 tokens.\"\n",
        "chunks = GPT2_PATTERN.findall(test_text)\n",
        "\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"\\nPre-tokenized chunks ({len(chunks)} chunks):\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Show spaces explicitly\n",
        "    display = chunk.replace(' ', 'âŽµ')\n",
        "    print(f\"  {i:2}: '{display}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Complete Encoder: Text â†’ Token IDs\n",
        "\n",
        "Now let's put it all together into a complete encoder!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'Hello, world!'\n",
            "Token IDs: [15496, 11, 995, 0]\n",
            "\n",
            "Token breakdown:\n",
            "   15496 â†’ 'Hello' (displayed: 'Hello')\n",
            "      11 â†’ ',' (displayed: ',')\n",
            "     995 â†’ 'Ä world' (displayed: 'âŽµworld')\n",
            "       0 â†’ '!' (displayed: '!')\n"
          ]
        }
      ],
      "source": [
        "# Cache BPE results for efficiency\n",
        "bpe_cache = {}\n",
        "\n",
        "def encode(text):\n",
        "    \"\"\"\n",
        "    Encode text into GPT-2 token IDs.\n",
        "    \n",
        "    Steps:\n",
        "    1. Split text into chunks using regex\n",
        "    2. Convert each chunk to byte-encoded form\n",
        "    3. Apply BPE to each chunk\n",
        "    4. Look up token IDs in vocabulary\n",
        "    \n",
        "    Returns:\n",
        "        list: Token IDs\n",
        "    \"\"\"\n",
        "    token_ids = []\n",
        "    \n",
        "    # Step 1: Pre-tokenize using regex\n",
        "    chunks = GPT2_PATTERN.findall(text)\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        # Step 2: Convert to byte-encoded string\n",
        "        byte_encoded = text_to_bytes(chunk)\n",
        "        \n",
        "        # Step 3: Apply BPE (with caching)\n",
        "        if byte_encoded not in bpe_cache:\n",
        "            bpe_cache[byte_encoded] = bpe(byte_encoded)\n",
        "        bpe_tokens = bpe_cache[byte_encoded]\n",
        "        \n",
        "        # Step 4: Look up token IDs\n",
        "        for token in bpe_tokens:\n",
        "            if token in vocab:\n",
        "                token_ids.append(vocab[token])\n",
        "            else:\n",
        "                # This shouldn't happen with proper BPE, but handle gracefully\n",
        "                print(f\"Warning: Unknown token '{token}'\")\n",
        "    \n",
        "    return token_ids\n",
        "\n",
        "\n",
        "# Test it!\n",
        "test_text = \"Hello, world!\"\n",
        "token_ids = encode(test_text)\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"\\nToken breakdown:\")\n",
        "for tid in token_ids:\n",
        "    token_str = vocab_inverse[tid]\n",
        "    display = token_str.replace(\"Ä \", \"âŽµ\")\n",
        "    print(f\"  {tid:6} â†’ '{token_str}' (displayed: '{display}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Decoder: Token IDs â†’ Text\n",
        "\n",
        "Decoding is simpler than encoding:\n",
        "1. Look up each token ID â†’ token string\n",
        "2. Concatenate all token strings\n",
        "3. Convert byte-encoded string back to text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  'Hello, world! How are you today?'\n",
            "Encoded:   [15496, 11, 995, 0, 1374, 389, 345, 1909, 30]\n",
            "Decoded:   'Hello, world! How are you today?'\n",
            "\n",
            "Roundtrip successful: True âœ“\n"
          ]
        }
      ],
      "source": [
        "def decode(token_ids):\n",
        "    \"\"\"\n",
        "    Decode token IDs back into text.\n",
        "    \n",
        "    Steps:\n",
        "    1. Look up each token ID â†’ token string\n",
        "    2. Concatenate all strings\n",
        "    3. Convert from byte-encoded characters back to UTF-8 text\n",
        "    \n",
        "    Returns:\n",
        "        str: Decoded text\n",
        "    \"\"\"\n",
        "    # Step 1 & 2: Look up and concatenate token strings\n",
        "    byte_encoded_text = ''.join(vocab_inverse[tid] for tid in token_ids)\n",
        "    \n",
        "    # Step 3: Convert byte-encoded characters back to bytes, then decode as UTF-8\n",
        "    byte_values = [byte_decoder[c] for c in byte_encoded_text]\n",
        "    text = bytes(byte_values).decode('utf-8', errors='replace')\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "# Test roundtrip: encode then decode\n",
        "original = \"Hello, world! How are you today?\"\n",
        "encoded = encode(original)\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(f\"Original:  '{original}'\")\n",
        "print(f\"Encoded:   {encoded}\")\n",
        "print(f\"Decoded:   '{decoded}'\")\n",
        "print(f\"\\nRoundtrip successful: {original == decoded} âœ“\" if original == decoded else \"\\nâŒ Roundtrip failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Visualizing Tokenization\n",
        "\n",
        "Let's create a nice visualization to see how different texts get tokenized!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ Text: Hello, world!                                                  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Tokens: 4                                                            â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ [15496:'Hello'] [11:','] [995:' world'] [0:'!']                      â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ Text: The quick brown fox jumps over the lazy dog.                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Tokens: 10                                                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ [464:'The'] [2068:' quick'] [7586:' brown'] [21831:' fox']           â”‚\n",
            "â”‚ [18045:' jumps'] [625:' over'] [262:' the'] [16931:' lazy']          â”‚\n",
            "â”‚ [3290:' dog'] [13:'.']                                               â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ Text: GPT-2 uses 50257 tokens in its vocabulary.                     â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Tokens: 12                                                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ [38:'G'] [11571:'PT'] [12:'-'] [17:'2'] [3544:' uses'] [2026:' 50']  â”‚\n",
            "â”‚ [28676:'257'] [16326:' tokens'] [287:' in'] [663:' its']             â”‚\n",
            "â”‚ [25818:' vocabulary'] [13:'.']                                       â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ Text: Supercalifragilisticexpialidocious                             â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Tokens: 11                                                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ [12442:'Super'] [9948:'cal'] [361:'if'] [22562:'rag'] [346:'il']     â”‚\n",
            "â”‚ [396:'ist'] [501:'ice'] [42372:'xp'] [498:'ial'] [312:'id']          â”‚\n",
            "â”‚ [32346:'ocious']                                                     â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def visualize_tokenization(text):\n",
        "    \"\"\"\n",
        "    Visualize how text gets tokenized.\n",
        "    \n",
        "    Shows:\n",
        "    - Each token with its ID\n",
        "    - Token boundaries in the original text\n",
        "    - Number of tokens used\n",
        "    \"\"\"\n",
        "    token_ids = encode(text)\n",
        "    \n",
        "    print(f\"â”Œ{'â”€' * 70}â”\")\n",
        "    print(f\"â”‚ Text: {text[:62]:<62} â”‚\")\n",
        "    print(f\"â”œ{'â”€' * 70}â”¤\")\n",
        "    print(f\"â”‚ Tokens: {len(token_ids):<60} â”‚\")\n",
        "    print(f\"â”œ{'â”€' * 70}â”¤\")\n",
        "    \n",
        "    # Show each token\n",
        "    token_display = []\n",
        "    for tid in token_ids:\n",
        "        token_str = vocab_inverse[tid]\n",
        "        # Decode the token for display\n",
        "        try:\n",
        "            decoded = bytes([byte_decoder[c] for c in token_str]).decode('utf-8')\n",
        "        except:\n",
        "            decoded = token_str\n",
        "        display = repr(decoded)[1:-1]  # Remove quotes, keep escapes visible\n",
        "        token_display.append((tid, display))\n",
        "    \n",
        "    # Print tokens in rows\n",
        "    row = \"â”‚ \"\n",
        "    for tid, display in token_display:\n",
        "        token_repr = f\"[{tid}:'{display}']\"\n",
        "        if len(row) + len(token_repr) + 2 > 71:\n",
        "            print(f\"{row:<71}â”‚\")\n",
        "            row = \"â”‚ \"\n",
        "        row += token_repr + \" \"\n",
        "    if len(row) > 2:\n",
        "        print(f\"{row:<71}â”‚\")\n",
        "    \n",
        "    print(f\"â””{'â”€' * 70}â”˜\")\n",
        "\n",
        "\n",
        "# Try some examples!\n",
        "examples = [\n",
        "    \"Hello, world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"GPT-2 uses 50257 tokens in its vocabulary.\",\n",
        "    \"Supercalifragilisticexpialidocious\",\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    visualize_tokenization(example)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exploring Token Efficiency\n",
        "\n",
        "Let's see how efficient tokenization is for different types of text!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type                  Chars  Tokens  Chars/Token\n",
            "--------------------------------------------------\n",
            "Common English           74      17         4.35\n",
            "Technical code           73      31         2.35\n",
            "Numbers                  70      21         3.33\n",
            "Rare words               64      20         3.20\n",
            "Repetition               39      10         3.90\n",
            "All caps                 38      12         3.17\n"
          ]
        }
      ],
      "source": [
        "def analyze_tokenization(text):\n",
        "    \"\"\"Analyze tokenization efficiency.\"\"\"\n",
        "    token_ids = encode(text)\n",
        "    chars = len(text)\n",
        "    tokens = len(token_ids)\n",
        "    ratio = chars / tokens if tokens > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        \"text\": text[:50] + \"...\" if len(text) > 50 else text,\n",
        "        \"chars\": chars,\n",
        "        \"tokens\": tokens,\n",
        "        \"chars_per_token\": ratio\n",
        "    }\n",
        "\n",
        "\n",
        "# Test different types of text\n",
        "test_cases = [\n",
        "    (\"Common English\", \"The quick brown fox jumps over the lazy dog. It was a beautiful sunny day.\"),\n",
        "    (\"Technical code\", \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\"),\n",
        "    (\"Numbers\", \"The year 2024 had 365 days, and 2025 will have 365 too. Pi is 3.14159.\"),\n",
        "    (\"Rare words\", \"Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease.\"),\n",
        "    (\"Repetition\", \"the the the the the the the the the the\"),\n",
        "    (\"All caps\", \"HELLO WORLD THIS IS ALL UPPERCASE TEXT\"),\n",
        "]\n",
        "\n",
        "print(f\"{'Type':<20} {'Chars':>6} {'Tokens':>7} {'Chars/Token':>12}\")\n",
        "print(\"-\" * 50)\n",
        "for name, text in test_cases:\n",
        "    result = analyze_tokenization(text)\n",
        "    print(f\"{name:<20} {result['chars']:>6} {result['tokens']:>7} {result['chars_per_token']:>12.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Special Tokens\n",
        "\n",
        "GPT-2 has some special tokens worth knowing about:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special and interesting tokens:\n",
            "\n",
            "  End of text token: '<|endoftext|>' â†’ 50256\n",
            "\n",
            "Some common word tokens:\n",
            "  'âŽµthe' â†’ 262\n",
            "  'âŽµa' â†’ 257\n",
            "  'âŽµand' â†’ 290\n",
            "  'âŽµto' â†’ 284\n",
            "  'âŽµof' â†’ 286\n",
            "  'âŽµin' â†’ 287\n",
            "  'âŽµis' â†’ 318\n",
            "  'âŽµthat' â†’ 326\n",
            "  'âŽµit' â†’ 340\n",
            "\n",
            "Some single-letter tokens (first 10):\n",
            "  'a' â†’ 64\n",
            "  'b' â†’ 65\n",
            "  'c' â†’ 66\n",
            "  'd' â†’ 67\n",
            "  'e' â†’ 68\n",
            "  'f' â†’ 69\n",
            "  'g' â†’ 70\n",
            "  'h' â†’ 71\n",
            "  'i' â†’ 72\n",
            "  'j' â†’ 73\n"
          ]
        }
      ],
      "source": [
        "# Find some interesting tokens\n",
        "print(\"Special and interesting tokens:\\n\")\n",
        "\n",
        "# The end-of-text token\n",
        "eot_token = \"<|endoftext|>\"\n",
        "if eot_token in vocab:\n",
        "    print(f\"  End of text token: '{eot_token}' â†’ {vocab[eot_token]}\")\n",
        "\n",
        "print(\"\\nSome common word tokens:\")\n",
        "common_words = [\"Ä the\", \"Ä a\", \"Ä and\", \"Ä to\", \"Ä of\", \"Ä in\", \"Ä is\", \"Ä that\", \"Ä it\"]\n",
        "for word in common_words:\n",
        "    if word in vocab:\n",
        "        display = word.replace(\"Ä \", \"âŽµ\")\n",
        "        print(f\"  '{display}' â†’ {vocab[word]}\")\n",
        "\n",
        "print(\"\\nSome single-letter tokens (first 10):\")\n",
        "for i, char in enumerate(\"abcdefghij\"):\n",
        "    if char in vocab:\n",
        "        print(f\"  '{char}' â†’ {vocab[char]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Putting It All Together: A Complete Tokenizer Class\n",
        "\n",
        "Let's wrap everything into a clean, reusable class!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPT2Tokenizer created with 50,257 tokens\n"
          ]
        }
      ],
      "source": [
        "class GPT2Tokenizer:\n",
        "    \"\"\"\n",
        "    A from-scratch implementation of GPT-2's BPE tokenizer.\n",
        "    \n",
        "    This implements the full tokenization pipeline:\n",
        "    - Byte-level encoding\n",
        "    - Regex-based pre-tokenization  \n",
        "    - BPE merging\n",
        "    - Vocabulary lookup\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_path=\"models/gpt2/vocab.json\", merges_path=\"models/gpt2/merges.txt\"):\n",
        "        # Load vocabulary\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.vocab = json.load(f)\n",
        "        self.vocab_inverse = {v: k for k, v in self.vocab.items()}\n",
        "        \n",
        "        # Load merge rules\n",
        "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            merges_text = f.read()\n",
        "        merge_lines = [line for line in merges_text.split(\"\\n\")[1:] if line]\n",
        "        self.bpe_merges = {}\n",
        "        for i, line in enumerate(merge_lines):\n",
        "            parts = line.split(\" \")\n",
        "            if len(parts) == 2:\n",
        "                self.bpe_merges[tuple(parts)] = i\n",
        "        \n",
        "        # Create byte encoder/decoder\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        \n",
        "        # Pre-tokenization pattern\n",
        "        self.pattern = regex.compile(\n",
        "            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        )\n",
        "        \n",
        "        # BPE cache\n",
        "        self.cache = {}\n",
        "    \n",
        "    def _get_pairs(self, word):\n",
        "        \"\"\"Get all adjacent pairs in a word.\"\"\"\n",
        "        return set((word[i], word[i + 1]) for i in range(len(word) - 1))\n",
        "    \n",
        "    def _bpe(self, token):\n",
        "        \"\"\"Apply BPE to a single token.\"\"\"\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        \n",
        "        word = list(token)\n",
        "        \n",
        "        while len(word) > 1:\n",
        "            pairs = self._get_pairs(word)\n",
        "            if not pairs:\n",
        "                break\n",
        "            \n",
        "            best_pair = min(pairs, key=lambda p: self.bpe_merges.get(p, float('inf')))\n",
        "            if best_pair not in self.bpe_merges:\n",
        "                break\n",
        "            \n",
        "            first, second = best_pair\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "        \n",
        "        result = tuple(word)\n",
        "        self.cache[token] = result\n",
        "        return result\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token IDs.\"\"\"\n",
        "        token_ids = []\n",
        "        \n",
        "        for chunk in self.pattern.findall(text):\n",
        "            # Convert to byte-encoded string\n",
        "            byte_encoded = ''.join(self.byte_encoder[b] for b in chunk.encode('utf-8'))\n",
        "            \n",
        "            # Apply BPE\n",
        "            bpe_tokens = self._bpe(byte_encoded)\n",
        "            \n",
        "            # Look up token IDs\n",
        "            for token in bpe_tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "        \n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"Convert token IDs back to text.\"\"\"\n",
        "        text = ''.join(self.vocab_inverse[tid] for tid in token_ids)\n",
        "        byte_values = [self.byte_decoder[c] for c in text]\n",
        "        return bytes(byte_values).decode('utf-8', errors='replace')\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return vocabulary size.\"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "# Create our tokenizer!\n",
        "tokenizer = GPT2Tokenizer()\n",
        "print(f\"âœ… GPT2Tokenizer created with {len(tokenizer):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing GPT2Tokenizer:\n",
            "============================================================\n",
            "\n",
            "Original: 'Hello, world!'\n",
            "Encoded:  [15496, 11, 995, 0]\n",
            "Decoded:  'Hello, world!'\n",
            "Match: âœ“\n",
            "\n",
            "Original: 'The quick brown fox jumps over the lazy dog.'\n",
            "Encoded:  [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Decoded:  'The quick brown fox jumps over the lazy dog.'\n",
            "Match: âœ“\n",
            "\n",
            "Original: 'GPT-2 is a language model with 124M parameters.'\n",
            "Encoded:  [38, 11571, 12, 17, 318, 257, 3303, 2746, 351, 19755, 44, 10007, 13]\n",
            "Decoded:  'GPT-2 is a language model with 124M parameters.'\n",
            "Match: âœ“\n",
            "\n",
            "Original: 'Let's test contractions: I'm, you're, they've, won't.'\n",
            "Encoded:  [5756, 338, 1332, 2775, 507, 25, 314, 1101, 11, 345, 821, 11, 484, 1053, 11, 1839, 470, 13]\n",
            "Decoded:  'Let's test contractions: I'm, you're, they've, won't.'\n",
            "Match: âœ“\n"
          ]
        }
      ],
      "source": [
        "# Test the complete tokenizer\n",
        "test_texts = [\n",
        "    \"Hello, world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"GPT-2 is a language model with 124M parameters.\",\n",
        "    \"Let's test contractions: I'm, you're, they've, won't.\",\n",
        "]\n",
        "\n",
        "print(\"Testing GPT2Tokenizer:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text in test_texts:\n",
        "    encoded = tokenizer.encode(text)\n",
        "    decoded = tokenizer.decode(encoded)\n",
        "    \n",
        "    print(f\"\\nOriginal: '{text}'\")\n",
        "    print(f\"Encoded:  {encoded}\")\n",
        "    print(f\"Decoded:  '{decoded}'\")\n",
        "    print(f\"Match: {'âœ“' if text == decoded else 'âœ—'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2 Complete!\n",
        "\n",
        "We've successfully built a **complete BPE tokenizer from scratch**!\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **Why BPE?** - Balances vocabulary size with handling rare words\n",
        "2. **Byte Encoding** - GPT-2 maps bytes to printable Unicode characters\n",
        "3. **Pre-tokenization** - Regex splits text before BPE\n",
        "4. **BPE Algorithm** - Greedily merge highest-priority pairs\n",
        "5. **Vocabulary Lookup** - Convert BPE tokens to IDs\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "- `G` with dot = space prefix - Distinguishes \"the\" from \" the\"\n",
        "- Common words = single tokens - \"the\", \"and\", \"is\" are each one token\n",
        "- Rare words = multiple subwords - \"tokenization\" becomes \"token\" + \"ization\"\n",
        "- Vocabulary size: 50,257 - A sweet spot for English + code + symbols\n",
        "\n",
        "### Next Step:\n",
        "\n",
        "Now that we can convert text to numbers, we will learn how to **embed these numbers into vectors** that the model can process! We will explore token embeddings and positional encodings in Step 3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
