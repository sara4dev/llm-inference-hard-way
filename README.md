# LLM Inference the Hard Way

Learn how Large Language Model inference *actually* works by implementing it from scratch â€” no high-level libraries, just raw math and tensors.

## What's This About?

Instead of calling `model.generate()` and treating the model as a black box, we'll build every piece ourselves:

- **Parse raw model weights** directly from SafeTensors files
- **Implement tokenization** from scratch (BPE algorithm)
- **Build the Transformer** layer by layer
- **Understand Q, K, V matrices** and how attention really works
- **Generate text** token by token (autoregressive decoding)
- **Optimize with KV caching** for efficient inference

We use GPT-2 (124M parameters) as our learning model â€” small enough to run on a CPU, yet architecturally identical to modern LLMs like GPT-4.

---

## ğŸ—ºï¸ Learning Plan

Each step is a **Jupyter notebook** with explanations, visualizations, and runnable code.

### Step 1: Download & Explore the Model âœ…
> **Notebook:** `step1_download_model.ipynb`

- Download GPT-2 weights from HuggingFace
- Parse SafeTensors format manually (no library!)
- Understand tensors, token embeddings, and position embeddings
- Learn about attention heads and the full architecture

**Key insight:** The `c_attn.weight [768, 2304]` tensor produces Q, K, V together (2304 = 768 Ã— 3)

---

### Step 2: Build the Tokenizer ğŸ”¤
> **Notebook:** `step2_tokenizer.ipynb`

- Implement Byte Pair Encoding (BPE) from scratch
- Load `vocab.json` and `merges.txt`
- Encode: text â†’ token IDs
- Decode: token IDs â†’ text

**Key concepts:**
- BPE starts with bytes, merges common pairs
- `"Hello"` â†’ `[15496]` (single token)
- `"tokenization"` â†’ `["token", "ization"]` â†’ `[30001, 1634]`

---

### Step 3: Embeddings & Positional Encoding ğŸ“
> **Notebook:** `step3_embeddings.ipynb`

- Token embeddings: `wte.weight [50257, 768]`
- Position embeddings: `wpe.weight [1024, 768]`
- Combine: `hidden = token_emb + position_emb`

**Key insight:** Unlike modern models (RoPE), GPT-2 uses learned positional embeddings

---

### Step 4: Attention Mechanism Deep Dive ğŸ¯
> **Notebook:** `step4_attention.ipynb`

The heart of the transformer! We'll implement:

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

- **Q (Query):** "What am I looking for?"
- **K (Key):** "What do I contain?"
- **V (Value):** "What information do I provide?"

**Key concepts:**
- Multi-head attention: 12 heads Ã— 64 dims = 768
- Causal masking: Can't attend to future tokens
- Attention patterns visualization

---

### Step 5: The Full Transformer Block ğŸ§±
> **Notebook:** `step5_transformer_block.ipynb`

Put it together:
```
x = x + Attention(LayerNorm(x))  # Residual + attention
x = x + MLP(LayerNorm(x))        # Residual + feedforward
```

Components:
- LayerNorm (pre-norm architecture)
- Multi-head self-attention
- MLP (expand 4Ã—, GELU, project back)
- Residual connections

---

### Step 6: Forward Pass & Logits ğŸ“Š
> **Notebook:** `step6_forward_pass.ipynb`

- Stack 12 transformer blocks
- Final LayerNorm
- Project to vocabulary: `logits = hidden @ wte.weight.T`
- Understand weight tying (same matrix for input/output)

---

### Step 7: Autoregressive Generation ğŸ”„
> **Notebook:** `step7_generation.ipynb`

Generate text token by token:
```python
for _ in range(max_tokens):
    logits = forward(tokens)
    next_token = sample(logits[-1])
    tokens.append(next_token)
```

Sampling strategies:
- Greedy (argmax)
- Temperature scaling
- Top-k sampling
- Top-p (nucleus) sampling

---

### Step 8: KV Caching âš¡
> **Notebook:** `step8_kv_cache.ipynb`

Make inference fast!

**Problem:** Recomputing K, V for all previous tokens is wasteful

**Solution:** Cache K, V from previous steps
```python
# Without cache: O(nÂ²) per token
# With cache: O(n) per token
```

---

## Quick Start

```bash
# Clone the repo
git clone https://github.com/yourusername/llm-inference-hard-way.git
cd llm-inference-hard-way

# Install dependencies with uv
uv sync

# Launch Jupyter Lab
uv run jupyter lab

# Then open step1_download_model.ipynb and run the cells!
```

## Requirements

- Python 3.12+
- [uv](https://docs.astral.sh/uv/) for dependency management

### Dependencies

| Package | Purpose |
|---------|---------|
| `torch` | Tensor operations |
| `numpy` | Array manipulation |
| `requests` | Download model weights |
| `regex` | BPE tokenizer |
| `tqdm` | Progress bars |
| `jupyterlab` | Interactive notebooks |
| `ipykernel` | Jupyter Python kernel |

## GPT-2 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPT-2 (124M) Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Input: "Hello world"                                       â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚   Tokenizer     â”‚  "Hello world" â†’ [15496, 995]         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ Token Embedding â”‚  [50257, 768] lookup                   â”‚
â”‚  â”‚   + Position    â”‚  [1024, 768] lookup                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ Transformer Ã—12 â”‚  Each block:                           â”‚
â”‚  â”‚                 â”‚  â”œâ”€ LayerNorm                          â”‚
â”‚  â”‚                 â”‚  â”œâ”€ Multi-Head Attention (12 heads)    â”‚
â”‚  â”‚                 â”‚  â”œâ”€ Residual Connection                â”‚
â”‚  â”‚                 â”‚  â”œâ”€ LayerNorm                          â”‚
â”‚  â”‚                 â”‚  â”œâ”€ MLP (768â†’3072â†’768)                 â”‚
â”‚  â”‚                 â”‚  â””â”€ Residual Connection                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ Final LayerNorm â”‚                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚   LM Head       â”‚  hidden @ wte.T â†’ [50257] logits      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â†“                                                 â”‚
â”‚  Output: probability distribution over 50,257 tokens        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Hyperparameters:
  â€¢ n_vocab  = 50,257  (vocabulary size)
  â€¢ n_ctx    = 1,024   (max sequence length)
  â€¢ n_embd   = 768     (embedding dimension)
  â€¢ n_head   = 12      (attention heads)
  â€¢ n_layer  = 12      (transformer blocks)
  â€¢ d_head   = 64      (768 / 12, dimension per head)
```

## Project Structure

```
llm-inference-hard-way/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gpt2/                         # Downloaded model weights
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors         # 548 MB of weights
â”‚       â”œâ”€â”€ vocab.json                # Token â†’ ID mapping
â”‚       â””â”€â”€ merges.txt                # BPE merge rules
â”œâ”€â”€ step1_download_model.ipynb        # âœ… Download & explore
â”œâ”€â”€ step2_tokenizer.ipynb             # ğŸ”œ BPE tokenization
â”œâ”€â”€ step3_embeddings.ipynb            # ğŸ”œ Token + position embeddings
â”œâ”€â”€ step4_attention.ipynb             # ğŸ”œ Q, K, V and attention
â”œâ”€â”€ step5_transformer_block.ipynb     # ğŸ”œ Full transformer block
â”œâ”€â”€ step6_forward_pass.ipynb          # ğŸ”œ Complete forward pass
â”œâ”€â”€ step7_generation.ipynb            # ğŸ”œ Autoregressive decoding
â”œâ”€â”€ step8_kv_cache.ipynb              # ğŸ”œ KV caching optimization
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## Why "The Hard Way"?

Most tutorials use `transformers.AutoModel` or similar abstractions. While convenient, this hides the fascinating details:

- How does the model convert "Hello" into numbers?
- What exactly are Q, K, V in attention?
- Why does the model have 160 separate weight tensors?
- How does sampling work?
- Why is KV caching so important for inference speed?

By building from scratch, you'll truly understand what happens between input text and generated output.

## License

MIT
